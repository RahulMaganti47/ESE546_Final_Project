{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAML Impl",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5AFzvZ64FIyBgDRDuSNT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheilsarda/ESE546_Final_Project/blob/master/CAML_Impl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU02eDlC0_lh",
        "outputId": "2bed552e-a15c-452d-e1d1-8c22605c1a4e"
      },
      "source": [
        "#@title Mount Google Drive\r\n",
        "#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \r\n",
        "#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\r\n",
        "\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "#@title set up mount symlink\r\n",
        "\r\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\r\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\r\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\r\n",
        "  %mkdir $DRIVE_PATH\r\n",
        "\r\n",
        "## the space in `My Drive` causes some issues,\r\n",
        "## make a symlink to avoid this\r\n",
        "SYM_PATH = '/content/cs330_fall2020'\r\n",
        "if not os.path.exists(SYM_PATH):\r\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZwCtnkr1dmt",
        "outputId": "34b340e4-dd0c-4547-dafb-e078c56ab59f"
      },
      "source": [
        "#@title Install Requirements\r\n",
        "#@markdown Requirements for the assignment and display drivers\r\n",
        "\r\n",
        "# Robot sim\r\n",
        "!pip install gym==0.15.4\r\n",
        "!pip install pygame\r\n",
        "\r\n",
        "# Various things for render\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "# Rendering Environment\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install piglet\r\n",
        "!sudo apt-get install -y xvfb ffmpeg\r\n",
        "!pip install imageio\r\n",
        "!pip install PILLOW\r\n",
        "\r\n",
        "# Commented out IPython magic to ensure Python compatibility.\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "MJC_PATH = '{}/mujoco'.format(SYM_PATH)\r\n",
        "if not os.path.exists(MJC_PATH):\r\n",
        "  %mkdir $MJC_PATH\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\r\n",
        "  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\r\n",
        "  !unzip -q mujoco200_linux.zip\r\n",
        "  %mv mujoco200_linux mujoco200\r\n",
        "  %rm mujoco200_linux.zip\r\n",
        "\r\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\r\n",
        "\r\n",
        "## installation on colab does not find *.so files\r\n",
        "## in LD_LIBRARY_PATH, copy over manually instead\r\n",
        "!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/\r\n",
        "\r\n",
        "#@title Important system updates for mujoco-py\r\n",
        "!apt update \r\n",
        "!apt install -y --no-install-recommends \\\r\n",
        "        build-essential \\\r\n",
        "        curl \\\r\n",
        "        git \\\r\n",
        "        gnupg2 \\\r\n",
        "        make \\\r\n",
        "        cmake \\\r\n",
        "        ffmpeg \\\r\n",
        "        swig \\\r\n",
        "        libz-dev \\\r\n",
        "        unzip \\\r\n",
        "        zlib1g-dev \\\r\n",
        "        libglfw3 \\\r\n",
        "        libglfw3-dev \\\r\n",
        "        libxrandr2 \\\r\n",
        "        libxinerama-dev \\\r\n",
        "        libxi6 \\\r\n",
        "        libxcursor-dev \\\r\n",
        "        libgl1-mesa-dev \\\r\n",
        "        libgl1-mesa-glx \\\r\n",
        "        libglew-dev \\\r\n",
        "        libosmesa6-dev \\\r\n",
        "        lsb-release \\\r\n",
        "        ack-grep \\\r\n",
        "        patchelf \\\r\n",
        "        wget \\\r\n",
        "        xpra \\\r\n",
        "        xserver-xorg-dev \\\r\n",
        "        xvfb \\\r\n",
        "        python-opengl \\\r\n",
        "        ffmpeg > /dev/null 2>&1\r\n",
        "\r\n",
        "#@title Clone and install mujoco-py\r\n",
        "#@markdown Remember that you need to put the key in the appropriate location as described above\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists('mujoco-py'):\r\n",
        "  !git clone https://github.com/openai/mujoco-py.git\r\n",
        "%cd mujoco-py\r\n",
        "%pip install -e .\r\n",
        "\r\n",
        "## cythonize at the first import\r\n",
        "import mujoco_py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n",
            "Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py.\u001b[0m\n",
            "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.18.5)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.15)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.14.4)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.29.21)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n",
            "Installing collected packages: mujoco-py\n",
            "  Found existing installation: mujoco-py 2.0.2.13\n",
            "    Can't uninstall 'mujoco-py'. No files were found to uninstall.\n",
            "  Running setup.py develop for mujoco-py\n",
            "Successfully installed mujoco-py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWtQ6eKttUfw"
      },
      "source": [
        "!apt-get -qq -y install libnvtoolsext1 > /dev/null\r\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\r\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\r\n",
        "!pip -q install gym\r\n",
        "!pip -q install pyglet\r\n",
        "!pip -q install pyopengl\r\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINRUIpSpqZV"
      },
      "source": [
        "\"\"\"\r\n",
        "Imports\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "import gym\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import argparse \r\n",
        "import time \r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import deque\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import random\r\n",
        "from gym import wrappers\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1024, 768))\r\n",
        "display.start()\r\n",
        "import os\r\n",
        "# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\r\n",
        "\r\n",
        "import matplotlib.animation\r\n",
        "import numpy as np\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "# Note, you may get a few warnings regarding Tensorflow and xdpyinfo, these are to be expected\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\r\n",
        "from random import shuffle\r\n",
        "import sys\r\n",
        "from copy import deepcopy\r\n",
        "import warnings\r\n",
        "import math, random\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import torch \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.autograd as autograd  \r\n",
        "import torch.nn.functional as F\r\n",
        "import abc\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import distributions as td\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch\r\n",
        "import torch.nn as nn \r\n",
        "import torch.nn.functional as F \r\n",
        "import torch.optim as optim\r\n",
        "from torch.distributions import Categorical\r\n",
        "import gym\r\n",
        "from collections import deque\r\n",
        "import numpy as np \r\n",
        "from torch.distributions import Categorical\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.distributions import Normal \r\n",
        "import os \r\n",
        "import os\r\n",
        "import gym\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40) # error only\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "import glob\r\n",
        "import io\r\n",
        "import base64\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\r\n",
        "    !bash ../xvfb start\r\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9zLBRQ9334n"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "eps = 1e-6\r\n",
        "min_std = -20\r\n",
        "max_std = 2\r\n",
        "\r\n",
        "class Actor(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(Actor, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3mu = nn.Linear(256, action_space)\r\n",
        "        self.fc3std = nn.Linear(256, action_space)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        z = F.relu(self.fc1(x))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        mu = self.fc3mu(z)\r\n",
        "        std = self.fc3std(z)\r\n",
        "        std = torch.clamp(std, min_std, max_std)\r\n",
        "        return mu, std\r\n",
        "\r\n",
        "class QCritic(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(QCritic, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space+action_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 1)\r\n",
        "    \r\n",
        "    \r\n",
        "    def forward(self, state, action):\r\n",
        "        z = torch.cat([state, action], 1)\r\n",
        "        z = F.relu(self.fc1(z))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        z = self.fc3(z)\r\n",
        "        return z\r\n",
        "\r\n",
        "\r\n",
        "class SAC:\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space, hp=None, name='SAC'): \r\n",
        "        self.name = name\r\n",
        "        if hp is None:\r\n",
        "            self.tau = 0.005\r\n",
        "            self.lr = 3*(1e-4)\r\n",
        "            self.batch_size = 256\r\n",
        "            self.gamma = 0.99\r\n",
        "            self.max_action = 1\r\n",
        "            self.min_action = 0\r\n",
        "        else:\r\n",
        "            self.tau = hp['tau'] \r\n",
        "            self.lr = hp['lr'] \r\n",
        "            self.batch_size = hp['batch_size']\r\n",
        "            self.gamma = hp['gamma']\r\n",
        "            self.max_action = hp['max_action']\r\n",
        "            self.min_action = hp['min_action']\r\n",
        "        self.actor = Actor(state_space, action_space).to(device)\r\n",
        "        self.qcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.qcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\r\n",
        "        self.target_ent = -action_space\r\n",
        "        print(\"Target Entropy: {}\".format(self.target_ent))\r\n",
        "        print(\"Verify Device: {}\".format(device))\r\n",
        "        self.act_opt = torch.optim.Adam(params=self.actor.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt = torch.optim.Adam(params=self.qcritic.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt2 = torch.optim.Adam(params=self.qcritic2.parameters(), lr=self.lr)\r\n",
        "        self.alpha_opt = torch.optim.Adam([self.log_alpha], lr=self.lr)\r\n",
        "        self.action_scale = torch.tensor((self.max_action-self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "        self.action_bias = torch.tensor((self.max_action+self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "\r\n",
        "    def predict(self, x, pred=True, internalCall=False, test=False):\r\n",
        "        if test:\r\n",
        "            self.actor.eval()\r\n",
        "        if pred and not internalCall:\r\n",
        "            x = torch.from_numpy(x).float().to(device)\r\n",
        "        if pred:\r\n",
        "            with torch.no_grad():\r\n",
        "                #self.actor.eval()\r\n",
        "                mu, std = self.actor(x)\r\n",
        "            #print(\"Mu, Log_Std: {} {}\".format(mu, std))\r\n",
        "            #self.get_actor_mean()\r\n",
        "        else:\r\n",
        "            self.actor.train()\r\n",
        "            mu, std = self.actor(x)\r\n",
        "        \r\n",
        "        #Treat initial output std as log_std - prevent <= 0 std\r\n",
        "        std = std.exp()\r\n",
        "        act_dist = Normal(mu, std)\r\n",
        "        \r\n",
        "        u = act_dist.rsample()\r\n",
        "        action = F.tanh(u)*self.action_scale+self.action_bias\r\n",
        "        log_prob = act_dist.log_prob(u)\r\n",
        "        jacobian = torch.log((1-torch.square(action))+eps)\r\n",
        "        #if internalCall:\r\n",
        "        jacobian = jacobian.sum(1, keepdim=True)\r\n",
        "        log_prob -= jacobian\r\n",
        "        \r\n",
        "        if test:\r\n",
        "            action = F.tanh(mu)*self.action_scale+self.action_bias\r\n",
        "            self.actor.train()\r\n",
        "        #  Internalcall used in training, evaluation and data collection has default False\r\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\r\n",
        "        if internalCall:\r\n",
        "            return action, log_prob\r\n",
        "        else:\r\n",
        "            #print(\"Action: {} State: {}\".format(action, x))\r\n",
        "            #self.get_actor_mean()\r\n",
        "            return np.squeeze(action.cpu().numpy()), np.squeeze(log_prob.cpu().numpy())\r\n",
        "\r\n",
        "\r\n",
        "    def _update_target(self):\r\n",
        "        for q1, q1t in zip(self.qcritic.parameters(), self.tcritic.parameters()):\r\n",
        "            q1t.data *= (1-self.tau)\r\n",
        "            q1t.data += (self.tau)*q1.data\r\n",
        "        for q2, q2t in zip(self.qcritic2.parameters(), self.tcritic2.parameters()):\r\n",
        "            q2t.data *= (1-self.tau)\r\n",
        "            q2t.data += self.tau*q2.data\r\n",
        "\r\n",
        "\r\n",
        "    def train_step(self, replay_buffer, batch_size):\r\n",
        "        state_set, action_set, reward_set, nstate_set, logprob_set, done_set = replay_buffer.sample(self.batch_size)\r\n",
        "        state_set = torch.from_numpy(state_set).float().to(device)\r\n",
        "        action_set = torch.from_numpy(action_set).float().to(device)\r\n",
        "        reward_set = torch.from_numpy(reward_set).float().to(device)\r\n",
        "        nstate_set = torch.from_numpy(nstate_set).float().to(device)\r\n",
        "        done_set = torch.from_numpy(done_set).float().to(device)\r\n",
        "        logprob_set = torch.from_numpy(logprob_set).float().to(device)\r\n",
        "       \r\n",
        "        alpha = self.log_alpha.exp()\r\n",
        "        act, log_prob = self.predict(nstate_set, internalCall=True)\r\n",
        "        qOut = torch.min(self.tcritic(nstate_set, act).detach(), self.tcritic2(nstate_set, act).detach())\r\n",
        "        qOut -= alpha.detach()*(log_prob)\r\n",
        "        reward_set = torch.unsqueeze(reward_set, 1)\r\n",
        "        done_set = torch.unsqueeze(done_set, 1)\r\n",
        "        target = reward_set+(1-done_set)*self.gamma*qOut\r\n",
        "        q1loss = F.mse_loss(target, self.qcritic(state_set, action_set))\r\n",
        "        q2loss = F.mse_loss(target, self.qcritic2(state_set, action_set))\r\n",
        "        \r\n",
        "        self.qc_opt.zero_grad()\r\n",
        "        q1loss.backward()\r\n",
        "        self.qc_opt.step()\r\n",
        "        self.qc_opt2.zero_grad()\r\n",
        "        q2loss.backward()\r\n",
        "        self.qc_opt2.step()\r\n",
        "\r\n",
        "        self.act_opt.zero_grad()\r\n",
        "        act, log_prob = self.predict(state_set, False, True)\r\n",
        "        min_q = torch.min(self.qcritic(state_set, act), self.qcritic2(state_set, act))\r\n",
        "        actor_loss = (alpha.detach()*log_prob-min_q).mean() \r\n",
        "        actor_loss.backward()\r\n",
        "        self.act_opt.step()\r\n",
        "        \r\n",
        "        self.alpha_opt.zero_grad()\r\n",
        "        alphaLoss = (-self.log_alpha*(log_prob+self.target_ent).detach()).mean()\r\n",
        "        alphaLoss.backward()\r\n",
        "        self.alpha_opt.step()\r\n",
        "        self._update_target()\r\n",
        "        #print(\"Loss List: q1 {}, q2 {}, act {}, alpha {}\".format(q1loss, q2loss, actor_loss, alphaLoss)) \r\n",
        "        #self.get_actor_mean()\r\n",
        "\r\n",
        "    def save(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "            if os.path.isdir('models') is False:\r\n",
        "                os.mkdir('models')\r\n",
        "        torch.save({\r\n",
        "            'actor': self.actor.state_dict(),\r\n",
        "            'qcritic': self.qcritic.state_dict(),\r\n",
        "            'qcritic2': self.qcritic2.state_dict(),\r\n",
        "            'tcritic': self.tcritic.state_dict(),\r\n",
        "            'tcritic2': self.tcritic.state_dict(),\r\n",
        "            'log_alpha': self.log_alpha,\r\n",
        "            'qopt': self.qc_opt.state_dict(),\r\n",
        "            'qopt2': self.qc_opt2.state_dict(),\r\n",
        "            'actOpt': self.act_opt.state_dict(),\r\n",
        "            'alphaOpt': self.alpha_opt.state_dict()\r\n",
        "            }, path) \r\n",
        "    \r\n",
        "\r\n",
        "    def load(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "        load_dict = torch.load(path)\r\n",
        "        self.actor.load_state_dict(load_dict['actor'])\r\n",
        "        self.qcritic.load_state_dict(load_dict['qcritic'])\r\n",
        "        self.qcritic2.load_state_dict(load_dict['qcritic2'])\r\n",
        "        self.tcritic.load_state_dict(load_dict['tcritic'])\r\n",
        "        self.tcritic2.load_state_dict(load_dict['tcritic2'])\r\n",
        "        self.log_alpha = load_dict['log_alpha']\r\n",
        "        self.qc_opt.load_state_dict(load_dict['qopt'])\r\n",
        "        self.qc_opt2.load_state_dict(load_dict['qopt2'])\r\n",
        "        self.act_opt.load_state_dict(load['actOpt'])\r\n",
        "        self.alpha_opt.load_state_dict(load_dict['alphaOpt']) \r\n",
        "    \r\n",
        "    def get_actor_mean(self):\r\n",
        "        print(\"Start\")\r\n",
        "        for name, p in self.actor.named_parameters():\r\n",
        "            print(name)\r\n",
        "            print(p.data.mean())\r\n",
        "            print(\"Gradient Data\")\r\n",
        "            print(p.grad.data.mean())\r\n",
        "        print(\"End\")\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs_q1dW80yv7"
      },
      "source": [
        "class ReplayMemory(object):\r\n",
        "\r\n",
        "    def __init__(self, length = 10000):\r\n",
        "    \r\n",
        "        # Buffer Collection: (S, A, R, S', log_prob (if available),  D)\r\n",
        "        # Done represents a mask of either 0 and 1\r\n",
        "        self.length = length\r\n",
        "        self.buffer = []\r\n",
        "\r\n",
        "    def add(self, sample):\r\n",
        "\r\n",
        "        if (len(self.buffer) > self.length):\r\n",
        "            self.buffer.pop(0)\r\n",
        "        self.buffer.append(sample)\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        \r\n",
        "        idx = np.random.permutation(len(self.buffer))[:batch_size]\r\n",
        "        state_b = []\r\n",
        "        action_b = []\r\n",
        "        reward_b = []\r\n",
        "        nextstate_b = []\r\n",
        "        done_b = [] \r\n",
        "        log_prob = []\r\n",
        "        for i in idx:\r\n",
        "            if (len(self.buffer[0])==5):\r\n",
        "                s, a, r, sp, d = self.buffer[i]\r\n",
        "            else:\r\n",
        "                s, a, r, sp, d, lp = self.buffer[i]\r\n",
        "                log_prob.append(lp)\r\n",
        "            state_b.append(s)\r\n",
        "            action_b.append(a)\r\n",
        "            reward_b.append(r)\r\n",
        "            nextstate_b.append(sp)\r\n",
        "            done_b.append(d)\r\n",
        "        state_b = np.array(state_b)\r\n",
        "        action_b = np.array(action_b)\r\n",
        "        reward_b = np.array(reward_b)\r\n",
        "        nextstate_b = np.array(nextstate_b)\r\n",
        "        done_b = np.array(done_b)\r\n",
        "        if len(self.buffer[0]) == 5:\r\n",
        "            print(\"Red Flag\")\r\n",
        "            return (state_b, action_b, reward_b, nextstate_b, done_b)\r\n",
        "        else:\r\n",
        "            return (state_b, action_b, reward_b, nextstate_b, done_b, np.array(log_prob))\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Hml1od5taD"
      },
      "source": [
        "def clog_prob(val, mu=0, std=1):\r\n",
        "    return np.sum(-np.log(np.sqrt(2*np.pi))-np.log(std**2)-(1/2*std)*np.square(val-mu))\r\n",
        "\r\n",
        "def process_state(state):\r\n",
        "    \r\n",
        "    #state /= np.sqrt(np.sum(np.square(state)))\r\n",
        "    return state\r\n",
        "\r\n",
        "def fillBuffer(env, memory, action_space, length=1000, returnlp=True):\r\n",
        "    \r\n",
        "    state = process_state(env.reset())\r\n",
        "    for _ in range(length):\r\n",
        "        action = np.random.normal(size=action_space)\r\n",
        "        if returnlp:\r\n",
        "            log_prob = clog_prob(action)\r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "        new_state = process_state(new_state)\r\n",
        "        if returnlp:\r\n",
        "            memory.add((state, action, reward, new_state, log_prob, done))\r\n",
        "        else:\r\n",
        "            memory.add((state, action, reward, new_state, done))\r\n",
        "        state = new_state\r\n",
        "        if done:\r\n",
        "            state = process_state(env.reset())\r\n",
        "\r\n",
        "def recordEps(env, model, recordEps=True, save_path=None):\r\n",
        "\r\n",
        "    #  Records episode run by default, otherwise acts as a test run for the model\r\n",
        "    if recordEps:\r\n",
        "        rec = gym.wrappers.monitoring.video_recorder.VideoRecorder(env, save_path)\r\n",
        "    state = env.reset()\r\n",
        "    r = 0\r\n",
        "    step = 0\r\n",
        "    while(True and step <= 5000):\r\n",
        "        tup = model.predict(np.expand_dims(state, axis=0), test=True)\r\n",
        "        if (type(tup) is tuple):\r\n",
        "            action, _ = tup\r\n",
        "        else:\r\n",
        "            action = tup\r\n",
        "        if recordEps:\r\n",
        "            rec.capture_frame()\r\n",
        "        state, reward, done, _ = env.step(action)\r\n",
        "        r += reward\r\n",
        "        step += 1\r\n",
        "        if done:\r\n",
        "            if recordEps:\r\n",
        "                rec.close()\r\n",
        "            print(\"Reward at termination: {}\".format(r))\r\n",
        "            print(\"Avg Reward: {}\".format(r/step))\r\n",
        "            return\r\n",
        "\r\n",
        "def baseEp(env):\r\n",
        "\r\n",
        "    state = env.reset()\r\n",
        "    r = 0\r\n",
        "    while(True):\r\n",
        "        action = np.random.normal(size = env.action_space.shape[0])\r\n",
        "        # print(\"Generated random action\")\r\n",
        "        state, reward, done, _ = env.step(action)\r\n",
        "        r += reward\r\n",
        "        # print(\"Reward: \" + str(reward))\r\n",
        "        if done:\r\n",
        "            print(\"Random: Reward at Termination: {}\".format(r))\r\n",
        "            return\r\n",
        "\r\n",
        "\r\n",
        "def runEp(env, memory, model, returnlp=False, returnReward=False):\r\n",
        "     \r\n",
        "    step = 0\r\n",
        "    epochReward = []\r\n",
        "    stepList = []\r\n",
        "    epReward = 0\r\n",
        "    state = process_state(env.reset())\r\n",
        "    for _ in range(5000): # Note that 5000 was an arbitrary choice\r\n",
        "        if not returnlp:\r\n",
        "            action = model.predict(np.expand_dims(state, axis=0))\r\n",
        "        else:\r\n",
        "            action, log_prob = model.predict(np.expand_dims(state, axis=0))\r\n",
        "        \r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "        new_state = process_state(new_state)\r\n",
        "        \r\n",
        "        #  Add transition tuple to replay buffer\r\n",
        "        if not returnlp:\r\n",
        "            memory.add((state, action, reward, new_state, done))\r\n",
        "        else:\r\n",
        "            memory.add((state, action, reward, new_state, log_prob, done))\r\n",
        "        state = new_state\r\n",
        "        step += 1\r\n",
        "        \r\n",
        "        #  Train every 50 steps\r\n",
        "        if (step%50) == 0:\r\n",
        "            for _ in range(50):\r\n",
        "                model.train_step(memory, batch_size) \r\n",
        "        if returnReward:\r\n",
        "            epReward += reward\r\n",
        "        if done:\r\n",
        "            state = process_state(env.reset())\r\n",
        "            done = False\r\n",
        "            if returnReward:\r\n",
        "                stepList.append(step)\r\n",
        "                epochReward.append(epReward)\r\n",
        "                epReward = 0\r\n",
        "    if returnReward:\r\n",
        "        return stepList, epochReward, action\r\n",
        "    else: \r\n",
        "        return step\r\n",
        "\r\n",
        "def train_model(env, memory, model, epIter):\r\n",
        "    reward_plot = [] # Format of (time step, ep_reward) pairs\r\n",
        "    tStep = 0\r\n",
        "    for i in range(epIter):\r\n",
        "        if ((i+1)%10 == 0): \r\n",
        "            stepList, epochReward, act = runEp(env, memory, model, True, returnReward=True) \r\n",
        "            step = stepList[-1]\r\n",
        "            print(\"Step \" + str(step))\r\n",
        "            for j in range(len(stepList)):\r\n",
        "                stepList[j] += tStep\r\n",
        "            reward_plot.extend([[a,b] for a,b in zip(stepList, epochReward)])\r\n",
        "        else:\r\n",
        "            step = runEp(env, memory, model, returnlp=True, returnReward=False)\r\n",
        "            print(\"here\" + str(step))\r\n",
        "        if ((i+1)%20 == 0):\r\n",
        "            print(\"Last action on last episode: {}\".format(act))\r\n",
        "            print(\"Runtime (hours) at checkpoint: {}\".format((time.time()-start_time)/3600))\r\n",
        "        \r\n",
        "            model.save()\r\n",
        "        tStep += step\r\n",
        "    return reward_plot        "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJmLbDWozVT_"
      },
      "source": [
        "class Policy(nn.Module): \r\n",
        "    def __init__(self, s_size=6, h_size=50, a_size=3):\r\n",
        "        super(Policy, self).__init__()\r\n",
        "        self.l1 = nn.Linear(s_size, h_size)\r\n",
        "        self.l2 = nn.Linear(h_size, a_size)\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            self.l1, \r\n",
        "            nn.ReLU(), \r\n",
        "            self.l2, \r\n",
        "            nn.Softmax(dim=1)\r\n",
        "        )\r\n",
        "         \r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x)\r\n",
        "    \r\n",
        "    def act(self, state):\r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\r\n",
        "        probs = self.forward(state)\r\n",
        "        m = Categorical(probs)\r\n",
        "        action = m.sample()\r\n",
        "        return action.item() - 1, m.log_prob(action)\r\n",
        "\r\n",
        "def compute_rewards(rewards, gamma):\r\n",
        "    discounted_rewards = np.zeros(len(rewards))\r\n",
        "    moving_add = 0\r\n",
        "    for i in reversed(range(0, len(rewards))):\r\n",
        "        moving_add = moving_add*gamma + rewards[i]\r\n",
        "        discounted_rewards[i] = moving_add\r\n",
        "\r\n",
        "    return discounted_rewards\r\n",
        "\r\n",
        "class Critic(nn.Module): \r\n",
        "    def __init__(self, state_dim=6, hidden_dim=20, output_dim=1, lambd=.9):\r\n",
        "        super(Critic, self).__init__()\r\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim, bias=False)\r\n",
        "        self.l2 = nn.Linear(hidden_dim, output_dim, bias=False)\r\n",
        "        self.lambd = lambd\r\n",
        "\r\n",
        "    def forward(self, state): \r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0) \r\n",
        "        x = F.relu(self.l1(state)) \r\n",
        "        x = self.l2(x) \r\n",
        "        return F.softmax(x, dim=1)\r\n",
        "\r\n",
        "    def td_error(self, reward, value_next, value_now, gamma, done, I): \r\n",
        "        if done: I = I * gamma \r\n",
        "        td_error = reward + gamma*(1-done)*value_next - value_now \r\n",
        "        return td_error \r\n",
        "        \r\n",
        "def train(n_episodes, policy, critic, gamma, print_every=4):\r\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\r\n",
        "    optimizer_v = optim.Adam(critic.parameters(), lr=.001)\r\n",
        "    scores_deque = deque(maxlen=100)\r\n",
        "    \r\n",
        "    total_rewards = []\r\n",
        "    for ep in range(n_episodes): \r\n",
        "        traj_log_probs = []\r\n",
        "        rewards = []\r\n",
        "        state = env.reset()\r\n",
        "        score = 0 \r\n",
        "        I = 1.0 \r\n",
        "        done = False\r\n",
        " \r\n",
        "        while not done:\r\n",
        "            action, log_prob = policy.act(state)\r\n",
        "            #value_func = critic.forward(state)\r\n",
        "            traj_log_probs.append(log_prob)\r\n",
        "            next_state, reward, done, _ = env.step(action)\r\n",
        "            #value_func_next = critic(next_state)\r\n",
        "            #td_error = critic.td_error(reward, value_func_next, value_func, gamma, done, I) \r\n",
        "            \r\n",
        "            score += reward\r\n",
        "            rewards.append(reward)\r\n",
        "           \r\n",
        "        scores_deque.append(score) \r\n",
        "        total_rewards.append(score)\r\n",
        "                \r\n",
        "        disc_rewards = compute_rewards(rewards, gamma)\r\n",
        "        disc_rewards = torch.tensor(disc_rewards)\r\n",
        "        \r\n",
        "        policy_loss = [] \r\n",
        "        for t, log_prob in enumerate(traj_log_probs):\r\n",
        "            policy_loss.append(-log_prob * disc_rewards[t])  \r\n",
        "        policy_loss = torch.cat(policy_loss).sum() \r\n",
        "        \r\n",
        "        #value_loss = F.l1_loss(value, torch.tensor([disc_rewards]))\r\n",
        "        #add gradient trace \r\n",
        "        #for p in critic.parameters(): \r\n",
        "        #    p.grad = p.grad * critic.lambd\r\n",
        "        \r\n",
        "        #loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\r\n",
        "\r\n",
        "        # backprop\r\n",
        "        optimizer.zero_grad()\r\n",
        "        policy_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        if ep % print_every == 0:\r\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))        \r\n",
        "    \r\n",
        "    return total_rewards\r\n",
        "\r\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEK2oAl9-kpQ"
      },
      "source": [
        "from gym.wrappers import Monitor\r\n",
        "def wrap_env(env):\r\n",
        "    env = Monitor(env, './video', force=True)\r\n",
        "    return env\r\n",
        "\r\n",
        "def show_video():\r\n",
        "    mp4list = glob.glob('video/*.mp4')\r\n",
        "    if len(mp4list) > 0:\r\n",
        "        mp4 = mp4list[0]\r\n",
        "        video = io.open(mp4, 'r+b').read()\r\n",
        "        encoded = base64.b64encode(video)\r\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n",
        "                                                loop controls style=\"height: 400px;\">\r\n",
        "                                                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n",
        "                                            </video>'''.format(encoded.decode('ascii'))))\r\n",
        "    else: \r\n",
        "        print(\"Could not find video\")  "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1GeLoo7uMz",
        "outputId": "c156d92a-cc94-4906-f7fa-1be714f33294"
      },
      "source": [
        "start_time = time.time()\r\n",
        "envName = 'HalfCheetah-v2'\r\n",
        "batch_size = 256\r\n",
        "\r\n",
        "print(\"Cuda available: {}\".format(torch.cuda.is_available()))\r\n",
        "if torch.cuda.is_available():\r\n",
        "    print(\"Device Count: {}\".format(torch.cuda.device(0)))\r\n",
        "    print(\"Device Name (first): {}\".format(torch.cuda.get_device_name(0)))\r\n",
        "print(\"Environment Used: {}\".format(envName))\r\n",
        "env = gym.make(envName) \r\n",
        "action_space = env.action_space.shape[0]\r\n",
        "state_space = env.observation_space.shape[0] \r\n",
        "hp = {'tau': 0.005,\\\r\n",
        "        'lr': 3*(1e-4),\\\r\n",
        "        'batch_size': 256,\\\r\n",
        "        'gamma': 0.99,\\\r\n",
        "        'max_action': env.action_space.high,\\\r\n",
        "        'min_action': env.action_space.low\\\r\n",
        "        }\r\n",
        "model = SAC(state_space, action_space, hp=hp, name='DDPG_halfCheetahv2')\r\n",
        "\r\n",
        "rmemory = ReplayMemory(int(1e6))\r\n",
        "fillBuffer(env, rmemory, action_space)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available: True\n",
            "Device Count: <torch.cuda.device object at 0x7f7598773ba8>\n",
            "Device Name (first): Tesla P100-PCIE-16GB\n",
            "Environment Used: HalfCheetah-v2\n",
            "Target Entropy: -6\n",
            "Verify Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQT_BDYjATdr",
        "outputId": "4b5b9d9a-9a48-4c3c-8d87-33baf7a6fcd2"
      },
      "source": [
        "reward_plot = train_model(env, rmemory, model, 10)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "Step 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "6wywYhxteIY0",
        "outputId": "9fe05804-e3f9-432f-ce9f-4ae87b41229a"
      },
      "source": [
        "plt.plot([v[0] for v in reward_plot], [v[1] for v in reward_plot])\r\n",
        "plt.title(\"Episode Reward vs TimeStep\")\r\n",
        "plt.show()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yTFkUHx8QIt",
        "outputId": "b3d9a9e5-b166-4c86-bab5-7855ba146873"
      },
      "source": [
        "matplotlib.use('Agg')\r\n",
        "# env = wrap_env(env)\r\n",
        "\r\n",
        "baseEp(env)\r\n",
        "baseEp(env)\r\n",
        "baseEp(env)\r\n",
        "\r\n",
        "env.close()\r\n",
        "# show_video()\r\n",
        "\r\n",
        "# print(\"End runtime: {} seconds\".format(time.time()-start_time))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random: Reward at Termination: -652.8554101276853\n",
            "Random: Reward at Termination: -712.6995353010171\n",
            "Random: Reward at Termination: -584.7761032867407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLoduY0qLsQ"
      },
      "source": [
        "class DQN_Network(nn.Module):\r\n",
        "    def __init__(self): \r\n",
        "        super(DQN_Network, self).__init__() \r\n",
        "        \r\n",
        "        # __GRU__ \r\n",
        "        \"\"\"\r\n",
        "        self.input_layer = nn.Linear(8, 128)\r\n",
        "        self.hidden_1 = nn.Linear(128, 128)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.hidden_state = torch.tensor(torch.zeros(2,1,32))\r\n",
        "        self.rnn = nn.GRU(128, 32, 2)\r\n",
        "        self.action_head = nn.Linear(31, 5)\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        self.input_layer = nn.Linear(8, 32)\r\n",
        "        self.hidden_1 = nn.Linear(32, 32)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.output_layer = nn.Linear(31, 5)\r\n",
        "    \r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        state = state.squeeze()\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = torch.sigmoid(self.input_layer(state))\r\n",
        "        out = torch.tanh(self.hidden_1(out))\r\n",
        "        out, self.hidden_state = self.rnn(out.view(1,-1,128), self.hidden_state.data)\r\n",
        "        out = F.relu(self.hidden_2(out.squeeze()))\r\n",
        "        out = self.action_head(out)\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.sigmoid(self.output_layer(out))\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.relu(self.output_layer(out))\r\n",
        "        \"\"\"\r\n",
        "        return out \r\n",
        "\r\n",
        "class DQN_Agent(): \r\n",
        "    def __init__(self): \r\n",
        "        #initialize target and policy networks \r\n",
        "        self.target_net = DQN_Network()\r\n",
        "        self.policy_net = DQN_Network()\r\n",
        "\r\n",
        "        self.eps_start = .9  \r\n",
        "        self.eps_end = .05\r\n",
        "        self.eps_decay = 200  \r\n",
        "        self.steps_done = 0 \r\n",
        "\r\n",
        "    def select_action(self, state): \r\n",
        "        random_n = random.random() #generate random number\r\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\r\n",
        "            math.exp(-1. * self.steps_done / self.eps_decay)\r\n",
        "        self.steps_done += 1  \r\n",
        "\r\n",
        "        if (random_n < eps_threshold): \r\n",
        "        #take random action (random # betwee 0 and 4)\r\n",
        "            action = torch.tensor([random.randrange(4)]) \r\n",
        "        else: \r\n",
        "        #take the best action  \r\n",
        "            with torch.no_grad(): \r\n",
        "                actions = self.policy_net(state)  \r\n",
        "                action = torch.argmax(actions).view(1, 1) \r\n",
        "                \r\n",
        "        return action.item() \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIX7ac77ujgc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}