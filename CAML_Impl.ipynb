{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAML Impl",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9YPg6z4hOzMkkAWMlwHKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheilsarda/ESE546_Final_Project/blob/master/CAML_Impl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU02eDlC0_lh",
        "outputId": "1228960d-575b-4e12-b283-4ef2010b2ec2"
      },
      "source": [
        "#@title Mount Google Drive\r\n",
        "#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \r\n",
        "#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\r\n",
        "\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "#@title set up mount symlink\r\n",
        "\r\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\r\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\r\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\r\n",
        "  %mkdir $DRIVE_PATH\r\n",
        "\r\n",
        "## the space in `My Drive` causes some issues,\r\n",
        "## make a symlink to avoid this\r\n",
        "SYM_PATH = '/content/cs330_fall2020'\r\n",
        "if not os.path.exists(SYM_PATH):\r\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZwCtnkr1dmt",
        "outputId": "af4144e3-7417-4d75-ec6e-31e7c2d02c34"
      },
      "source": [
        "#@title Install Requirements\r\n",
        "#@markdown Requirements for the assignment and display drivers\r\n",
        "\r\n",
        "# Robot sim\r\n",
        "!pip install gym==0.15.4\r\n",
        "!pip install pygame\r\n",
        "\r\n",
        "# Various things for render\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "# Rendering Environment\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install piglet\r\n",
        "!sudo apt-get install -y xvfb ffmpeg\r\n",
        "!pip install imageio\r\n",
        "!pip install PILLOW\r\n",
        "\r\n",
        "# Commented out IPython magic to ensure Python compatibility.\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "MJC_PATH = '{}/mujoco'.format(SYM_PATH)\r\n",
        "if not os.path.exists(MJC_PATH):\r\n",
        "  %mkdir $MJC_PATH\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\r\n",
        "  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\r\n",
        "  !unzip -q mujoco200_linux.zip\r\n",
        "  %mv mujoco200_linux mujoco200\r\n",
        "  %rm mujoco200_linux.zip\r\n",
        "\r\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\r\n",
        "\r\n",
        "## installation on colab does not find *.so files\r\n",
        "## in LD_LIBRARY_PATH, copy over manually instead\r\n",
        "!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/\r\n",
        "\r\n",
        "#@title Important system updates for mujoco-py\r\n",
        "!apt update \r\n",
        "!apt install -y --no-install-recommends \\\r\n",
        "        build-essential \\\r\n",
        "        curl \\\r\n",
        "        git \\\r\n",
        "        gnupg2 \\\r\n",
        "        make \\\r\n",
        "        cmake \\\r\n",
        "        ffmpeg \\\r\n",
        "        swig \\\r\n",
        "        libz-dev \\\r\n",
        "        unzip \\\r\n",
        "        zlib1g-dev \\\r\n",
        "        libglfw3 \\\r\n",
        "        libglfw3-dev \\\r\n",
        "        libxrandr2 \\\r\n",
        "        libxinerama-dev \\\r\n",
        "        libxi6 \\\r\n",
        "        libxcursor-dev \\\r\n",
        "        libgl1-mesa-dev \\\r\n",
        "        libgl1-mesa-glx \\\r\n",
        "        libglew-dev \\\r\n",
        "        libosmesa6-dev \\\r\n",
        "        lsb-release \\\r\n",
        "        ack-grep \\\r\n",
        "        patchelf \\\r\n",
        "        wget \\\r\n",
        "        xpra \\\r\n",
        "        xserver-xorg-dev \\\r\n",
        "        xvfb \\\r\n",
        "        python-opengl \\\r\n",
        "        ffmpeg > /dev/null 2>&1\r\n",
        "\r\n",
        "#@title Clone and install mujoco-py\r\n",
        "#@markdown Remember that you need to put the key in the appropriate location as described above\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists('mujoco-py'):\r\n",
        "  !git clone https://github.com/openai/mujoco-py.git\r\n",
        "%cd mujoco-py\r\n",
        "%pip install -e .\r\n",
        "\r\n",
        "## cythonize at the first import\r\n",
        "import mujoco_py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWtQ6eKttUfw"
      },
      "source": [
        "!apt-get -qq -y install libnvtoolsext1 > /dev/null\r\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\r\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\r\n",
        "!pip -q install gym\r\n",
        "!pip -q install pyglet\r\n",
        "!pip -q install pyopengl\r\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINRUIpSpqZV"
      },
      "source": [
        "\"\"\"\r\n",
        "Imports\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "import gym\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import argparse \r\n",
        "import time \r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import deque\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import random\r\n",
        "from gym import wrappers\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1024, 768))\r\n",
        "display.start()\r\n",
        "import os\r\n",
        "# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\r\n",
        "\r\n",
        "import matplotlib.animation\r\n",
        "import numpy as np\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "# Note, you may get a few warnings regarding Tensorflow and xdpyinfo, these are to be expected\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\r\n",
        "from random import shuffle\r\n",
        "import sys\r\n",
        "from copy import deepcopy\r\n",
        "import warnings\r\n",
        "import math, random\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import torch \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.autograd as autograd  \r\n",
        "import torch.nn.functional as F\r\n",
        "import abc\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import distributions as td\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch\r\n",
        "import torch.nn as nn \r\n",
        "import torch.nn.functional as F \r\n",
        "import torch.optim as optim\r\n",
        "from torch.distributions import Categorical\r\n",
        "import gym\r\n",
        "from collections import deque\r\n",
        "import numpy as np \r\n",
        "from torch.distributions import Categorical\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.distributions import Normal \r\n",
        "import os "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9zLBRQ9334n"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "eps = 1e-6\r\n",
        "min_std = -20\r\n",
        "max_std = 2\r\n",
        "\r\n",
        "class Actor(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(Actor, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3mu = nn.Linear(256, action_space)\r\n",
        "        self.fc3std = nn.Linear(256, action_space)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        z = F.relu(self.fc1(x))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        mu = self.fc3mu(z)\r\n",
        "        std = self.fc3std(z)\r\n",
        "        std = torch.clamp(std, min_std, max_std)\r\n",
        "        return mu, std\r\n",
        "\r\n",
        "class QCritic(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(QCritic, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space+action_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 1)\r\n",
        "    \r\n",
        "    \r\n",
        "    def forward(self, state, action):\r\n",
        "        z = torch.cat([state, action], 1)\r\n",
        "        z = F.relu(self.fc1(z))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        z = self.fc3(z)\r\n",
        "        return z\r\n",
        "\r\n",
        "\r\n",
        "class SAC:\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space, hp=None, name='SAC'): \r\n",
        "        self.name = name\r\n",
        "        if hp is None:\r\n",
        "            self.tau = 0.005\r\n",
        "            self.lr = 3*(1e-4)\r\n",
        "            self.batch_size = 256\r\n",
        "            self.gamma = 0.99\r\n",
        "            self.max_action = 1\r\n",
        "            self.min_action = 0\r\n",
        "        else:\r\n",
        "            self.tau = hp['tau'] \r\n",
        "            self.lr = hp['lr'] \r\n",
        "            self.batch_size = hp['batch_size']\r\n",
        "            self.gamma = hp['gamma']\r\n",
        "            self.max_action = hp['max_action']\r\n",
        "            self.min_action = hp['min_action']\r\n",
        "        self.actor = Actor(state_space, action_space).to(device)\r\n",
        "        self.qcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.qcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\r\n",
        "        self.target_ent = -action_space\r\n",
        "        print(\"Target Entropy: {}\".format(self.target_ent))\r\n",
        "        print(\"Verify Device: {}\".format(device))\r\n",
        "        self.act_opt = torch.optim.Adam(params=self.actor.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt = torch.optim.Adam(params=self.qcritic.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt2 = torch.optim.Adam(params=self.qcritic2.parameters(), lr=self.lr)\r\n",
        "        self.alpha_opt = torch.optim.Adam([self.log_alpha], lr=self.lr)\r\n",
        "        self.action_scale = torch.tensor((self.max_action-self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "        self.action_bias = torch.tensor((self.max_action+self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "\r\n",
        "    def predict(self, x, pred=True, internalCall=False, test=False):\r\n",
        "        if test:\r\n",
        "            self.actor.eval()\r\n",
        "        if pred and not internalCall:\r\n",
        "            x = torch.from_numpy(x).float().to(device)\r\n",
        "        if pred:\r\n",
        "            with torch.no_grad():\r\n",
        "                #self.actor.eval()\r\n",
        "                mu, std = self.actor(x)\r\n",
        "            #print(\"Mu, Log_Std: {} {}\".format(mu, std))\r\n",
        "            #self.get_actor_mean()\r\n",
        "        else:\r\n",
        "            self.actor.train()\r\n",
        "            mu, std = self.actor(x)\r\n",
        "        \r\n",
        "        #Treat initial output std as log_std - prevent <= 0 std\r\n",
        "        std = std.exp()\r\n",
        "        act_dist = Normal(mu, std)\r\n",
        "        \r\n",
        "        u = act_dist.rsample()\r\n",
        "        action = F.tanh(u)*self.action_scale+self.action_bias\r\n",
        "        log_prob = act_dist.log_prob(u)\r\n",
        "        jacobian = torch.log((1-torch.square(action))+eps)\r\n",
        "        #if internalCall:\r\n",
        "        jacobian = jacobian.sum(1, keepdim=True)\r\n",
        "        log_prob -= jacobian\r\n",
        "        \r\n",
        "        if test:\r\n",
        "            action = F.tanh(mu)*self.action_scale+self.action_bias\r\n",
        "            self.actor.train()\r\n",
        "        #  Internalcall used in training, evaluation and data collection has default False\r\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\r\n",
        "        if internalCall:\r\n",
        "            return action, log_prob\r\n",
        "        else:\r\n",
        "            #print(\"Action: {} State: {}\".format(action, x))\r\n",
        "            #self.get_actor_mean()\r\n",
        "            return np.squeeze(action.cpu().numpy()), np.squeeze(log_prob.cpu().numpy())\r\n",
        "\r\n",
        "\r\n",
        "    def _update_target(self):\r\n",
        "        for q1, q1t in zip(self.qcritic.parameters(), self.tcritic.parameters()):\r\n",
        "            q1t.data *= (1-self.tau)\r\n",
        "            q1t.data += (self.tau)*q1.data\r\n",
        "        for q2, q2t in zip(self.qcritic2.parameters(), self.tcritic2.parameters()):\r\n",
        "            q2t.data *= (1-self.tau)\r\n",
        "            q2t.data += self.tau*q2.data\r\n",
        "\r\n",
        "\r\n",
        "    def train_step(self, replay_buffer, batch_size):\r\n",
        "        state_set, action_set, reward_set, nstate_set, logprob_set, done_set = replay_buffer.sample(self.batch_size)\r\n",
        "        state_set = torch.from_numpy(state_set).float().to(device)\r\n",
        "        action_set = torch.from_numpy(action_set).float().to(device)\r\n",
        "        reward_set = torch.from_numpy(reward_set).float().to(device)\r\n",
        "        nstate_set = torch.from_numpy(nstate_set).float().to(device)\r\n",
        "        done_set = torch.from_numpy(done_set).float().to(device)\r\n",
        "        logprob_set = torch.from_numpy(logprob_set).float().to(device)\r\n",
        "       \r\n",
        "        alpha = self.log_alpha.exp()\r\n",
        "        act, log_prob = self.predict(nstate_set, internalCall=True)\r\n",
        "        qOut = torch.min(self.tcritic(nstate_set, act).detach(), self.tcritic2(nstate_set, act).detach())\r\n",
        "        qOut -= alpha.detach()*(log_prob)\r\n",
        "        reward_set = torch.unsqueeze(reward_set, 1)\r\n",
        "        done_set = torch.unsqueeze(done_set, 1)\r\n",
        "        target = reward_set+(1-done_set)*self.gamma*qOut\r\n",
        "        q1loss = F.mse_loss(target, self.qcritic(state_set, action_set))\r\n",
        "        q2loss = F.mse_loss(target, self.qcritic2(state_set, action_set))\r\n",
        "        \r\n",
        "        self.qc_opt.zero_grad()\r\n",
        "        q1loss.backward()\r\n",
        "        self.qc_opt.step()\r\n",
        "        self.qc_opt2.zero_grad()\r\n",
        "        q2loss.backward()\r\n",
        "        self.qc_opt2.step()\r\n",
        "\r\n",
        "        self.act_opt.zero_grad()\r\n",
        "        act, log_prob = self.predict(state_set, False, True)\r\n",
        "        min_q = torch.min(self.qcritic(state_set, act), self.qcritic2(state_set, act))\r\n",
        "        actor_loss = (alpha.detach()*log_prob-min_q).mean() \r\n",
        "        actor_loss.backward()\r\n",
        "        self.act_opt.step()\r\n",
        "        \r\n",
        "        self.alpha_opt.zero_grad()\r\n",
        "        alphaLoss = (-self.log_alpha*(log_prob+self.target_ent).detach()).mean()\r\n",
        "        alphaLoss.backward()\r\n",
        "        self.alpha_opt.step()\r\n",
        "        self._update_target()\r\n",
        "        #print(\"Loss List: q1 {}, q2 {}, act {}, alpha {}\".format(q1loss, q2loss, actor_loss, alphaLoss)) \r\n",
        "        #self.get_actor_mean()\r\n",
        "\r\n",
        "    def save(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "            if os.path.isdir('models') is False:\r\n",
        "                os.mkdir('models')\r\n",
        "        torch.save({\r\n",
        "            'actor': self.actor.state_dict(),\r\n",
        "            'qcritic': self.qcritic.state_dict(),\r\n",
        "            'qcritic2': self.qcritic2.state_dict(),\r\n",
        "            'tcritic': self.tcritic.state_dict(),\r\n",
        "            'tcritic2': self.tcritic.state_dict(),\r\n",
        "            'log_alpha': self.log_alpha,\r\n",
        "            'qopt': self.qc_opt.state_dict(),\r\n",
        "            'qopt2': self.qc_opt2.state_dict(),\r\n",
        "            'actOpt': self.act_opt.state_dict(),\r\n",
        "            'alphaOpt': self.alpha_opt.state_dict()\r\n",
        "            }, path) \r\n",
        "    \r\n",
        "\r\n",
        "    def load(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "        load_dict = torch.load(path)\r\n",
        "        self.actor.load_state_dict(load_dict['actor'])\r\n",
        "        self.qcritic.load_state_dict(load_dict['qcritic'])\r\n",
        "        self.qcritic2.load_state_dict(load_dict['qcritic2'])\r\n",
        "        self.tcritic.load_state_dict(load_dict['tcritic'])\r\n",
        "        self.tcritic2.load_state_dict(load_dict['tcritic2'])\r\n",
        "        self.log_alpha = load_dict['log_alpha']\r\n",
        "        self.qc_opt.load_state_dict(load_dict['qopt'])\r\n",
        "        self.qc_opt2.load_state_dict(load_dict['qopt2'])\r\n",
        "        self.act_opt.load_state_dict(load['actOpt'])\r\n",
        "        self.alpha_opt.load_state_dict(load_dict['alphaOpt']) \r\n",
        "    \r\n",
        "    def get_actor_mean(self):\r\n",
        "        print(\"Start\")\r\n",
        "        for name, p in self.actor.named_parameters():\r\n",
        "            print(name)\r\n",
        "            print(p.data.mean())\r\n",
        "            print(\"Gradient Data\")\r\n",
        "            print(p.grad.data.mean())\r\n",
        "        print(\"End\")\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs_q1dW80yv7"
      },
      "source": [
        "class ReplayMemory(object):\r\n",
        "\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"Saves a transition.\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[self.position] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "HJmLbDWozVT_",
        "outputId": "05480315-a172-46ee-b700-7f34fe0db0a5"
      },
      "source": [
        "class Policy(nn.Module): \r\n",
        "    def __init__(self, s_size=6, h_size=50, a_size=3):\r\n",
        "        super(Policy, self).__init__()\r\n",
        "        self.l1 = nn.Linear(s_size, h_size)\r\n",
        "        self.l2 = nn.Linear(h_size, a_size)\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            self.l1, \r\n",
        "            nn.ReLU(), \r\n",
        "            self.l2, \r\n",
        "            nn.Softmax(dim=1)\r\n",
        "        )\r\n",
        "         \r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x)\r\n",
        "    \r\n",
        "    def act(self, state):\r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\r\n",
        "        probs = self.forward(state)\r\n",
        "        m = Categorical(probs)\r\n",
        "        action = m.sample()\r\n",
        "        return action.item() - 1, m.log_prob(action)\r\n",
        "\r\n",
        "def compute_rewards(rewards, gamma):\r\n",
        "    discounted_rewards = np.zeros(len(rewards))\r\n",
        "    moving_add = 0\r\n",
        "    for i in reversed(range(0, len(rewards))):\r\n",
        "        moving_add = moving_add*gamma + rewards[i]\r\n",
        "        discounted_rewards[i] = moving_add\r\n",
        "\r\n",
        "    return discounted_rewards\r\n",
        "\r\n",
        "class Critic(nn.Module): \r\n",
        "    def __init__(self, state_dim=6, hidden_dim=20, output_dim=1, lambd=.9):\r\n",
        "        super(Critic, self).__init__()\r\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim, bias=False)\r\n",
        "        self.l2 = nn.Linear(hidden_dim, output_dim, bias=False)\r\n",
        "        self.lambd = lambd\r\n",
        "\r\n",
        "    def forward(self, state): \r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0) \r\n",
        "        x = F.relu(self.l1(state)) \r\n",
        "        x = self.l2(x) \r\n",
        "        return F.softmax(x, dim=1)\r\n",
        "\r\n",
        "    def td_error(self, reward, value_next, value_now, gamma, done, I): \r\n",
        "        if done: I = I * gamma \r\n",
        "        td_error = reward + gamma*(1-done)*value_next - value_now \r\n",
        "        return td_error \r\n",
        "        \r\n",
        "def train(n_episodes, policy, critic, gamma, print_every=4):\r\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\r\n",
        "    optimizer_v = optim.Adam(critic.parameters(), lr=.001)\r\n",
        "    scores_deque = deque(maxlen=100)\r\n",
        "    \r\n",
        "    total_rewards = []\r\n",
        "    for ep in range(n_episodes): \r\n",
        "        traj_log_probs = []\r\n",
        "        rewards = []\r\n",
        "        state = env.reset()\r\n",
        "        score = 0 \r\n",
        "        I = 1.0 \r\n",
        "        done = False\r\n",
        " \r\n",
        "        while not done:\r\n",
        "            action, log_prob = policy.act(state)\r\n",
        "            #value_func = critic.forward(state)\r\n",
        "            traj_log_probs.append(log_prob)\r\n",
        "            next_state, reward, done, _ = env.step(action)\r\n",
        "            #value_func_next = critic(next_state)\r\n",
        "            #td_error = critic.td_error(reward, value_func_next, value_func, gamma, done, I) \r\n",
        "            \r\n",
        "            score += reward\r\n",
        "            rewards.append(reward)\r\n",
        "           \r\n",
        "        scores_deque.append(score) \r\n",
        "        total_rewards.append(score)\r\n",
        "                \r\n",
        "        disc_rewards = compute_rewards(rewards, gamma)\r\n",
        "        disc_rewards = torch.tensor(disc_rewards)\r\n",
        "        \r\n",
        "        policy_loss = [] \r\n",
        "        for t, log_prob in enumerate(traj_log_probs):\r\n",
        "            policy_loss.append(-log_prob * disc_rewards[t])  \r\n",
        "        policy_loss = torch.cat(policy_loss).sum() \r\n",
        "        \r\n",
        "        #value_loss = F.l1_loss(value, torch.tensor([disc_rewards]))\r\n",
        "        #add gradient trace \r\n",
        "        #for p in critic.parameters(): \r\n",
        "        #    p.grad = p.grad * critic.lambd\r\n",
        "        \r\n",
        "        #loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\r\n",
        "\r\n",
        "        # backprop\r\n",
        "        optimizer.zero_grad()\r\n",
        "        policy_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        if ep % print_every == 0:\r\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))        \r\n",
        "    \r\n",
        "    return total_rewards\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    start_time = time.time()\r\n",
        "    envName = 'HalfCheetah-v2'\r\n",
        "    print(\"Cuda available: {}\".format(torch.cuda.is_available()))\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        print(\"Device Count: {}\".format(torch.cuda.device(0)))\r\n",
        "        print(\"Device Name (first): {}\".format(torch.cuda.get_device_name(0)))\r\n",
        "    print(\"Environment Used: {}\".format(envName))\r\n",
        "    env = gym.make(envName) \r\n",
        "    action_space = env.action_space.shape[0]\r\n",
        "    state_space = env.observation_space.shape[0] \r\n",
        "    hp = {'tau': 0.005,\\\r\n",
        "            'lr': 3*(1e-4),\\\r\n",
        "            'batch_size': 256,\\\r\n",
        "            'gamma': 0.99,\\\r\n",
        "            'max_action': env.action_space.high,\\\r\n",
        "            'min_action': env.action_space.low\\\r\n",
        "            }\r\n",
        "    model = SAC(state_space, action_space, hp=hp, name='DDPG_halfCheetahv2')\r\n",
        "    if args.test is False:\r\n",
        "        rmemory = ReplayBuffer(int(1e6))\r\n",
        "        #fillBuffer(env, rmemory, action_space)\r\n",
        "        train_model(env, rmemory, model, 200)\r\n",
        "        recordEps(env, model, 'vid/DDPGhalfCheetah.mp4')\r\n",
        "        print(\"Baseline comparison\")\r\n",
        "        for _ in range(5):\r\n",
        "            baseEp(env)\r\n",
        "    else:\r\n",
        "        model.load('models/DDPG_cheetah')\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            recordEps(env, model, False, 'vid/halfCheetah.mp4')\r\n",
        "        else:\r\n",
        "            recordEps(env, model, False, 'vid/halfCheetah.mp4')\r\n",
        "        for _ in range(5):\r\n",
        "            baseEp(env)\r\n",
        "    print(\"End runtime: {} seconds\".format(time.time()-start_time))\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available: True\n",
            "Device Count: <torch.cuda.device object at 0x7fdd047c5b38>\n",
            "Device Name (first): Tesla T4\n",
            "Environment Used: HalfCheetah-v2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mujoco_py'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-718282c4e750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Device Name (first): {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Environment Used: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mstate_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/mujoco/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMujocoEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ^^^^^ so that user gets the correct error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# message if mujoco is not installed correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAntEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_cheetah\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHalfCheetahEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mDEFAULT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLoduY0qLsQ"
      },
      "source": [
        "class DQN_Network(nn.Module):\r\n",
        "    def __init__(self): \r\n",
        "        super(DQN_Network, self).__init__() \r\n",
        "        \r\n",
        "        # __GRU__ \r\n",
        "        \"\"\"\r\n",
        "        self.input_layer = nn.Linear(8, 128)\r\n",
        "        self.hidden_1 = nn.Linear(128, 128)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.hidden_state = torch.tensor(torch.zeros(2,1,32))\r\n",
        "        self.rnn = nn.GRU(128, 32, 2)\r\n",
        "        self.action_head = nn.Linear(31, 5)\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        self.input_layer = nn.Linear(8, 32)\r\n",
        "        self.hidden_1 = nn.Linear(32, 32)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.output_layer = nn.Linear(31, 5)\r\n",
        "    \r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        state = state.squeeze()\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = torch.sigmoid(self.input_layer(state))\r\n",
        "        out = torch.tanh(self.hidden_1(out))\r\n",
        "        out, self.hidden_state = self.rnn(out.view(1,-1,128), self.hidden_state.data)\r\n",
        "        out = F.relu(self.hidden_2(out.squeeze()))\r\n",
        "        out = self.action_head(out)\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.sigmoid(self.output_layer(out))\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.relu(self.output_layer(out))\r\n",
        "        \"\"\"\r\n",
        "        return out \r\n",
        "\r\n",
        "class DQN_Agent(): \r\n",
        "    def __init__(self): \r\n",
        "        #initialize target and policy networks \r\n",
        "        self.target_net = DQN_Network()\r\n",
        "        self.policy_net = DQN_Network()\r\n",
        "\r\n",
        "        self.eps_start = .9  \r\n",
        "        self.eps_end = .05\r\n",
        "        self.eps_decay = 200  \r\n",
        "        self.steps_done = 0 \r\n",
        "\r\n",
        "    def select_action(self, state): \r\n",
        "        random_n = random.random() #generate random number\r\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\r\n",
        "            math.exp(-1. * self.steps_done / self.eps_decay)\r\n",
        "        self.steps_done += 1  \r\n",
        "\r\n",
        "        if (random_n < eps_threshold): \r\n",
        "        #take random action (random # betwee 0 and 4)\r\n",
        "            action = torch.tensor([random.randrange(4)]) \r\n",
        "        else: \r\n",
        "        #take the best action  \r\n",
        "            with torch.no_grad(): \r\n",
        "                actions = self.policy_net(state)  \r\n",
        "                action = torch.argmax(actions).view(1, 1) \r\n",
        "                \r\n",
        "        return action.item() \r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIX7ac77ujgc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}