{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS330 Homework Stencil.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwmQwFeQo2qW"
      },
      "source": [
        "## CS 330 Homework 3 Installation\n",
        "\n",
        "The following code blocks will install the required libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7xAlJMrHwA"
      },
      "source": [
        "## Setup for Google Drive and Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa0Ri_edrNIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf7f4e4-e400-412d-f46d-36e434fd0b87"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \n",
        "#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#@title set up mount symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs330_fall2020'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-raFHMIpUun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3ccb50-8657-4cab-a321-36b7b8fe7ccb"
      },
      "source": [
        "#@title Install Requirements\n",
        "#@markdown Requirements for the assignment and display drivers\n",
        "\n",
        "# Robot sim\n",
        "!pip install gym==0.15.4\n",
        "!pip install pygame\n",
        "\n",
        "# Various things for render\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "\n",
        "# Rendering Environment\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install imageio\n",
        "!pip install PILLOW"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.8MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp36-none-any.whl size=1648486 sha256=9236b7d31e86da823ada5c8c09bd118ffef111e58e8a2080ffff89f0e19ac062\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/4c/2ebe8ab1a695a446574bc48d96eb3503649893be8c769e7fafd65fd18833/pygame-2.0.0-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5MB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 2s (301 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 2s (358 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 147220 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yTRSgI-ryd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d02f43b-31c3-4901-a16f-9cde4d9198e1"
      },
      "source": [
        "#@title Download Mujoco from an online repository\n",
        "\n",
        "MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n",
        "if not os.path.exists(MJC_PATH):\n",
        "  %mkdir $MJC_PATH\n",
        "%cd $MJC_PATH\n",
        "if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n",
        "  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n",
        "  !unzip -q mujoco200_linux.zip\n",
        "  %mv mujoco200_linux mujoco200\n",
        "  %rm mujoco200_linux.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQJfUoanr3pm"
      },
      "source": [
        "#@title Important: ACTION Required BEFORE running this cell\n",
        "#@markdown Place the mujoco key we have given you into a text file called mjkey.txt \n",
        "#@markdown and ensure that the mujoco key is in the Google Drive path `cs330_fall2020/mujoco`.\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n",
        "os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n",
        "\n",
        "## installation on colab does not find *.so files\n",
        "## in LD_LIBRARY_PATH, copy over manually instead\n",
        "!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYMDKI8hrHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504d21ae-e171-4944-9afb-8a5dc3a1e852"
      },
      "source": [
        "#@title Important system updates for mujoco-py\n",
        "!apt update \n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        xvfb \\\n",
        "        python-opengl \\\n",
        "        ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Fetched 44.3 kB in 2s (23.3 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcG4cdIysCu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e3b477-8843-4ca5-a196-bd001d6b736a"
      },
      "source": [
        "#@title Clone and install mujoco-py\n",
        "#@markdown Remember that you need to put the key in the appropriate location as described above\n",
        "%cd $MJC_PATH\n",
        "if not os.path.exists('mujoco-py'):\n",
        "  !git clone https://github.com/openai/mujoco-py.git\n",
        "%cd mujoco-py\n",
        "%pip install -e .\n",
        "\n",
        "## cythonize at the first import\n",
        "import mujoco_py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n",
            "Cloning into 'mujoco-py'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 2148 (delta 69), reused 55 (delta 30), pack-reused 2051\u001b[K\n",
            "Receiving objects: 100% (2148/2148), 5.61 MiB | 2.68 MiB/s, done.\n",
            "Resolving deltas: 100% (1339/1339), done.\n",
            "Checking out files: 100% (200/200), done.\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n",
            "Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py.\u001b[0m\n",
            "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glfw>=1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/12/72/3190b7cc8494c05d7fb350237d3f51abdaff79e74d1e13d6f84df4b57f6b/glfw-2.0.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl\n",
            "Collecting fasteners~=0.15\n",
            "  Using cached https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.18.5)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.14.4)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.29.21)\n",
            "Collecting monotonic>=0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n",
            "Installing collected packages: glfw, monotonic, fasteners, mujoco-py\n",
            "  Running setup.py develop for mujoco-py\n",
            "Successfully installed fasteners-0.15 glfw-2.0.0 monotonic-1.5 mujoco-py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You appear to be missing a License Key for mujoco.  We expected to find the\n",
            "file here: /content/cs330_fall2020/mujoco/mjkey.txt\n",
            "\n",
            "You can get licenses at this page:\n",
            "\n",
            "    https://www.roboti.us/license.html\n",
            "\n",
            "If python tries to activate an invalid license, the process will exit.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-230f06b567fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m## cythonize at the first import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcymj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mujoco_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjrenderpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMjRenderPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m \u001b[0mmujoco_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscover_mujoco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0mcymj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cython_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmujoco_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/utils.py\u001b[0m in \u001b[0;36mdiscover_mujoco\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMISSING_KEY_MESSAGE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmujoco_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: \nYou appear to be missing a License Key for mujoco.  We expected to find the\nfile here: /content/cs330_fall2020/mujoco/mjkey.txt\n\nYou can get licenses at this page:\n\n    https://www.roboti.us/license.html\n\nIf python tries to activate an invalid license, the process will exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGGFqjdcsX3g"
      },
      "source": [
        "#@title Clone and install multiworld\n",
        "%cd $SYM_PATH\n",
        "!git clone https://github.com/vitchyr/multiworld.git\n",
        "\n",
        "%cd multiworld\n",
        "%pip install -e .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6Swy46pzyg"
      },
      "source": [
        "#@title Sets up virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zc9_-pRitwk"
      },
      "source": [
        "#@title Check imports and add helper functions for display\n",
        "\n",
        "import os\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3uYa0KHs0LB"
      },
      "source": [
        "#@title After running, you should see a video play\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "env = wrap_env(gym.make(\"Ant-v2\"))\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(10):\n",
        "    env.render(mode='rgb_array')\n",
        "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
        "    if term:\n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "print('Loading video...')\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6uncjbtGb_s"
      },
      "source": [
        "#@title Random seed is set to be fixed\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(330)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mD5hrg3cPn7"
      },
      "source": [
        "# BitFlip Goal Conditioned RL\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpGZxJIg9ghR"
      },
      "source": [
        "## BitFlipEnv\n",
        "\n",
        "Familiarize yourself with what the bit flip environment does and what each method does.\n",
        "\n",
        "You do *NOT* need to modify the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKw1cs-ZiAjt"
      },
      "source": [
        "class BitFlipEnv():\n",
        "    '''bit flipping environment for reinforcement learning.\n",
        "    The environment is a 1D vector of binary values (state vector).\n",
        "    At each step, the actor can flip a single bit (0 to 1 or 1 to 0).\n",
        "    The goal is to flip bits until the state vector matches the\n",
        "    goal vector (also a 1D vector of binary values). At each step,\n",
        "    the actor receives a goal of 0 if the state and goal vector\n",
        "    do not match and a reward of 1 if the state and goal vector\n",
        "    match.\n",
        "\n",
        "    Internally the state and goal vector are a numpy array, which\n",
        "    allows the vectors to be printed by the show_goal and show_state\n",
        "    methods. When '''\n",
        "\n",
        "    def __init__(self, num_bits, verbose = False):\n",
        "        '''Initialize new instance of BitFlip class.\n",
        "        inputs: num_bits - number of bits in the environment; must\n",
        "                be an integer\n",
        "                verbose - prints state and goal vector after each\n",
        "                          step if True'''\n",
        "\n",
        "        # check that num_bits is a positive integer\n",
        "        if (num_bits < 0) or (type(num_bits) != type(0)):\n",
        "            print(\"Invalid number of bits -  must be positive integer\")\n",
        "            return\n",
        "\n",
        "        # number of bits in the environment\n",
        "        self.num_bits = num_bits\n",
        "        # randomly set the state vector\n",
        "        self.state_vector = np.random.randint(0, 2, num_bits)\n",
        "        # randomly set the goal vector\n",
        "        self.goal_vector = np.random.randint(0, 2, num_bits)\n",
        "        # whether to print debugging info\n",
        "        self.verbose = verbose\n",
        "        # TODO set dimensions of observation space\n",
        "        self.observation_space = self.state_vector\n",
        "        # TODO create action space; may use gym type\n",
        "        self.action_space = num_bits\n",
        "        # space of the goal vector\n",
        "        self.goal_space = self.goal_vector\n",
        "        # number of steps taken\n",
        "        self.steps = 0\n",
        "\n",
        "        return\n",
        "\n",
        "    def show_goal(self):\n",
        "        '''Returns the goal as a numpy array. Used for debugging.'''\n",
        "        return self.goal_vector\n",
        "\n",
        "    def show_state(self):\n",
        "        '''Returns the state as a numpy array. Used for debugging.'''\n",
        "        return self.state_vector\n",
        "\n",
        "    def reset(self):\n",
        "        '''resets the environment. Returns a reset state_vector\n",
        "        and goal_vector as tf tensors'''\n",
        "\n",
        "        # randomly reset both the state and the goal vectors\n",
        "        self.state_vector = np.random.randint(0, 2, self.num_bits)\n",
        "        self.goal_vector = np.random.randint(0, 2, self.num_bits)\n",
        "        self.steps = 0\n",
        "\n",
        "        # return as np array\n",
        "        return self.state_vector, self.goal_vector\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        '''take a step and flip one of the bits.\n",
        "\n",
        "        inputs: action - integer index of the bit to flip\n",
        "        outputs: state - new state_vector (tensor)\n",
        "                 reward - 0 if state != goal and 1 if state == goal\n",
        "                 done - boolean value indicating if the goal has been reached'''\n",
        "        self.steps += 1\n",
        "\n",
        "\n",
        "        if action < 0 or action >= self.num_bits:\n",
        "            # check argument is in range\n",
        "            print(\"Invalid action! Must be integer ranging from \\\n",
        "                0 to num_bits-1\")\n",
        "            return\n",
        "\n",
        "        # flip the bit with index action\n",
        "        if self.state_vector[action] == 1:\n",
        "            self.state_vector[action] = 0\n",
        "        else:\n",
        "            self.state_vector[action] = 1\n",
        "\n",
        "        # initial values of reward and done - may change\n",
        "        # depending on state and goal vectors\n",
        "        reward = 0\n",
        "        done = True\n",
        "\n",
        "        # check if state and goal vectors are identical\n",
        "        if False in (self.state_vector == self.goal_vector):\n",
        "            reward = -1\n",
        "            done = False\n",
        "\n",
        "        # print additional info if verbose mode is on\n",
        "        if self.verbose:\n",
        "            print(\"Bit flipped:   \", action)\n",
        "            print(\"Goal vector:   \", self.goal_vector)\n",
        "            print(\"Updated state: \", self.state_vector)\n",
        "            print(\"Reward:        \", reward)\n",
        "\n",
        "        if done:\n",
        "            #print(\"Solved in: \", self.steps)\n",
        "            pass\n",
        "\n",
        "        # return state as numpy arrays\n",
        "        # return goal_vector in info field\n",
        "        return np.copy(self.state_vector), reward, done, self.steps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZOi6L7Pdgd0"
      },
      "source": [
        "## Buffer\n",
        "Familiarize yourself with what the buffer does \n",
        "\n",
        "You do *NOT* need to modify the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J14LUXqwkC9e"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque \n",
        "\n",
        "class Buffer(object) :\n",
        "\n",
        "\tdef __init__(self,size,sample_size):\n",
        "\n",
        "\t\tself.size = size\n",
        "\t\tself.sample_size = sample_size\n",
        "\t\tself.buffer = deque()\n",
        "\n",
        "\tdef add(self,state,action,reward,next_state) :\n",
        "\t\tself.buffer.append((state,action,reward,next_state))\n",
        "\n",
        "\t\tif len(self.buffer) > self.size:\n",
        "\t\t\tself.buffer.popleft()\n",
        "\n",
        "\tdef sample(self) :\n",
        "\t\tif len(self.buffer) < self.sample_size:\n",
        "\t\t\tsamples = self.buffer\n",
        "\t\telse:\t\n",
        "\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n",
        "\t\t\n",
        "\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n",
        "\t\taction = np.array([arr[1] for arr in samples])\n",
        "\t\treward = np.array([arr[2] for arr in samples])\n",
        "\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n",
        "\n",
        "\t\treturn state, action, reward, next_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPtYOkhghoz-"
      },
      "source": [
        "## BitFlip Goal Condition RL and Training\n",
        "\n",
        "Implement the changes you need for Problems 1-3 here in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgoB2zVqi2G8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_bits):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    hidden_dim = 256\n",
        "    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n",
        "    self.out = tf.keras.layers.Dense(num_bits,activation = None)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    x = self.dense1(inputs)\n",
        "    return self.out(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXD2QYkHh56I"
      },
      "source": [
        "# ************   Helper functions    ************ #\n",
        "\n",
        "def updateTarget(model, target_model, tau=0.95) :\n",
        "    model_weights = model.get_weights()\n",
        "    target_weights = target_model.get_weights()\n",
        "    new_weights = []\n",
        "    for i, weight in enumerate(model_weights):\n",
        "      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n",
        "\n",
        "    target_model.set_weights(new_weights)\n",
        "\n",
        "def solve_environment(num_bits, model, bit_env, state, goal_state, total_reward):\n",
        "    '''attempt to solve the bit flipping environment using the current policy\n",
        "\n",
        "    inputs: num_bits - number of bits to be looped over \n",
        "        model - DQN to run prediction on\n",
        "        bit_env - environment for bitflip\n",
        "        state - current state\n",
        "        goal_state - desired state\n",
        "        total_reward - cumulative reward so far\n",
        "    '''\n",
        "    \n",
        "    # list for recording what happened in the episode\n",
        "    episode_experience = []\n",
        "    succeeded = False\n",
        "\n",
        "    for t in range(num_bits):\n",
        "      \n",
        "      # attempt to solve the state - number of steps given to solve the\n",
        "      # state is equal to the size of the vector\n",
        "      \n",
        "      # ======================== TODO modify code ========================\n",
        "      continue\n",
        "      # forward pass to find action\n",
        "\n",
        "      # add to the episode experience (what happened)\n",
        "\n",
        "      # calculate total reward\n",
        "\n",
        "      # update state\n",
        "\n",
        "      # mark that we've finished the episode and succeeded with training\n",
        "\n",
        "\n",
        "      # ========================      END TODO       ========================\n",
        "\n",
        "\n",
        "    return succeeded, episode_experience, total_reward\n",
        "\n",
        "def solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward):\n",
        "    '''attempt to solve the bit_flip env using no goal'''\n",
        "    \n",
        "    # list for recording what happened in the episode\n",
        "    episode_experience = []\n",
        "    succeeded = False\n",
        "\n",
        "    for t in range(num_bits):\n",
        "        # attempt to solve the state - number of steps given to solve the\n",
        "        # state is equal to the passed argument steps_per_episode.\n",
        "\n",
        "        inputs = state\n",
        "        inputs = np.expand_dims(inputs, axis=0)\n",
        "        # forward pass to find action\n",
        "        out = model(inputs)\n",
        "        action = np.argmax(out,axis = 1)\n",
        "        # take the action\n",
        "        next_state,reward,done, _ = bit_env.step(action)\n",
        "        # add to the episode experience (what happened)\n",
        "        episode_experience.append((state,action,reward,next_state,goal_state))\n",
        "        # calculate total reward\n",
        "        total_reward+=reward\n",
        "        # update state\n",
        "        state = next_state\n",
        "        # mark that we've finished the episode and succeeded with training\n",
        "        if done:\n",
        "            if succeeded:\n",
        "                continue\n",
        "            else:\n",
        "                succeeded = True\n",
        "\n",
        "\n",
        "\n",
        "    return succeeded, episode_experience, total_reward\n",
        "\n",
        "\n",
        "def update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER):\n",
        "    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n",
        "    buffer. When HER is used, relabeled experiences are also added to the replay buffer\n",
        "\n",
        "    inputs: num_bits - number of bits to be looped over \n",
        "            replay_buffer - the buffer to store past experience in\n",
        "            episode_experience - list of transitions from the last episode\n",
        "            HER -  type of hindsight experience replay to use\n",
        "    modifies: replay_buffer\n",
        "    outputs: None'''\n",
        "\n",
        "    for t in range(num_bits) :\n",
        "        # copy actual experience from episode_experience to replay_buffer\n",
        "\n",
        "        # ======================== TODO modify code ========================\n",
        "        s,a,r,s_,g = episode_experience[t]\n",
        "        # state\n",
        "        inputs = s\n",
        "        # next state\n",
        "        inputs_ = s_\n",
        "        # add to the replay buffer\n",
        "        replay_buffer.add(inputs,a,r,inputs_)\n",
        "\n",
        "        # when HER is used, each call to update_replay_buffer should add num_relabeled\n",
        "        # relabeled points to the replay buffer\n",
        "\n",
        "        if HER == 'None':\n",
        "            # HER not being used, so do nothing\n",
        "            pass\n",
        "\n",
        "        elif HER == 'final':\n",
        "            # final - relabel based on final state in episode\n",
        "            pass\n",
        "\n",
        "        elif HER == 'future':\n",
        "            # future - relabel based on future state. At each timestep t, relabel the\n",
        "            # goal with a randomly select timestep between t and the end of the\n",
        "            # episode\n",
        "            pass\n",
        "\n",
        "        elif HER == 'random':\n",
        "             # random - relabel based on a random state in the episode\n",
        "            pass\n",
        "\n",
        "        # ========================      END TODO       ========================\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid value for Her flag - HER not used\")\n",
        "    return\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL4Cedzih-eg"
      },
      "source": [
        "\n",
        "# ************   Main training loop    ************ #\n",
        "\n",
        "\n",
        "def flip_bits(num_bits, num_epochs, buffer_size = 1e6, batch_size = 128, \n",
        "              num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40, HER = \"None\"):\n",
        "    '''Main loop for running in the bit flipping environment. The DQN is\n",
        "    trained over num_epochs. In each epoch, the agent runs in the environment\n",
        "    num_episodes number of times. The Q-target and Q-policy networks are\n",
        "    updated at the end of each epoch. Within one episode, Q-policy attempts\n",
        "    to solve the environment and is limited to the same number as steps as the\n",
        "    size of the environment\n",
        "\n",
        "    inputs: HER - string specifying whether to use HER'''\n",
        "\n",
        "    print(\"Running bit flip environment with %d bits and HER policy: %s\" %(num_bits, HER))\n",
        "\n",
        "    # create bit flipping environment and replay buffer\n",
        "    bit_env = BitFlipEnv(num_bits)\n",
        "    replay_buffer = Buffer(buffer_size,batch_size)\n",
        "\n",
        "    # set up Q-policy (model) and Q-target (target_model)\n",
        "    model = Model(num_bits)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
        "    target_model = Model(num_bits)\n",
        "\n",
        "    # ======================== TODO modify code ========================\n",
        "    # modify to be goal conditioned\n",
        "    state, goal_state = bit_env.reset()      \n",
        "    inputs = state\n",
        "    \n",
        "    inputs = np.expand_dims(inputs, axis=0)  \n",
        "    model(inputs)\n",
        "    target_model(inputs)\n",
        "\n",
        "    # start by making Q-target and Q-policy the same\n",
        "    updateTarget(model, target_model, tau=0.0)\n",
        "    # ========================      END TODO       ========================\n",
        "\n",
        "\n",
        "    total_loss = []                  # training loss for each epoch\n",
        "    success_rate = []                # success rate for each epoch\n",
        "    \n",
        "    for i in range(num_epochs):\n",
        "        # Run for a fixed number of epochs\n",
        "\n",
        "        total_reward = 0.0           # total reward for the epoch\n",
        "        successes = []               # record success rate for each episode of the epoch\n",
        "        losses = []                  # loss at the end of each epoch\n",
        "\n",
        "        for k in range(num_episodes):\n",
        "            # Run in the environment for num_episodes  \n",
        "            state, goal_state = bit_env.reset()             # reset the environment     \n",
        "            # attempt to solve the environment\n",
        "            # ======================== TODO modify code ========================\n",
        "            # modify to be goal conditioned\n",
        "            succeeded, episode_experience, total_reward = solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward)\n",
        "            # ========================     END TODO     ========================\n",
        "            successes.append(succeeded)                     # track whether we succeeded in environment \n",
        "            update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n",
        "        for k in range(opt_steps):\n",
        "            # optimize the Q-policy network\n",
        "\n",
        "            # sample from the replay buffer\n",
        "            state,action,reward,next_state = replay_buffer.sample()\n",
        "            # forward pass through target network   \n",
        "            # target_net_Q = sess.run(target_model.out,feed_dict = {target_model.inp : next_state})\n",
        "            with tf.GradientTape() as tape:\n",
        "              target_net_Q = target_model(next_state)\n",
        "              # calculate target reward\n",
        "              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n",
        "              # calculate predictions and loss\n",
        "              model_predict = model(state)\n",
        "              model_action_taken = np.reshape(action,[-1])\n",
        "              action_one_hot = tf.one_hot(model_action_taken, num_bits)\n",
        "              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n",
        "              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n",
        "              losses.append(loss)\n",
        "            \n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n",
        "        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n",
        "                \n",
        "    return success_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaqw28hxi6Zi"
      },
      "source": [
        "# Sample commands have been provided to you below\n",
        "# run with type of HER specified\n",
        "success_rate  = flip_bits(num_bits=7, num_epochs=150, HER='None') \n",
        "# pass success rate for each run as first argument and labels as second list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lua6jJR-cnv2"
      },
      "source": [
        "# Sawyer Environment Goal-Conditioned RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ojAJB_4colS"
      },
      "source": [
        "#@title Buffer\n",
        "#@markdown Same as the Buffer class before but placed here in the event you want to run the sections separately\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque \n",
        "\n",
        "class Buffer(object) :\n",
        "\n",
        "\tdef __init__(self,size,sample_size):\n",
        "\n",
        "\t\tself.size = size\n",
        "\t\tself.sample_size = sample_size\n",
        "\t\tself.buffer = deque()\n",
        "\n",
        "\tdef add(self,state,action,reward,next_state) :\n",
        "\t\tself.buffer.append((state,action,reward,next_state))\n",
        "\n",
        "\t\tif len(self.buffer) > self.size:\n",
        "\t\t\tself.buffer.popleft()\n",
        "\n",
        "\tdef sample(self) :\n",
        "\t\tif len(self.buffer) < self.sample_size:\n",
        "\t\t\tsamples = self.buffer\n",
        "\t\telse:\t\n",
        "\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n",
        "\t\t\n",
        "\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n",
        "\t\taction = np.array([arr[1] for arr in samples])\n",
        "\t\treward = np.array([arr[2] for arr in samples])\n",
        "\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n",
        "\n",
        "\t\treturn state, action, reward, next_state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4b_b31-lQFp"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import multiworld\n",
        "import glfw\n",
        "\n",
        "multiworld.register_all_envs()   \n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_act):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    hidden_dim = 256\n",
        "    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n",
        "    self.out = tf.keras.layers.Dense(num_act,activation = None)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVtTlZFSlSi2"
      },
      "source": [
        "# ************   Helper functions    ************ #\n",
        "# Globals\n",
        "\n",
        "NUM_DIM = 2\n",
        "NUM_ACT = 4\n",
        "done_threshold = -0.01\n",
        "Sawyer_Env = env = wrap_env(gym.make('SawyerReachXYEnv-v1'))\n",
        "\n",
        "def updateTarget(model, target_model, tau=0.95) :\n",
        "    model_weights = model.get_weights()\n",
        "    target_weights = target_model.get_weights()\n",
        "    new_weights = []\n",
        "    for i, weight in enumerate(model_weights):\n",
        "      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n",
        "\n",
        "    target_model.set_weights(new_weights)\n",
        "\n",
        "def take_action(action, render):\n",
        "    '''passes the discrete action selected by the Q-network to the Sawyer Arm.\n",
        "    The function returns the next state, the reward, and whether the environment\n",
        "    was solved. The environment done returned is not the same as the environment\n",
        "    done returned by the Sawyer environment. Due to discretization, it may not be\n",
        "    possible to exactly reach the goal. The done flag returns true if the end\n",
        "    state is within done_threshold of the final goal\n",
        "\n",
        "    inputs:  action - integer (0 to NUM_ACT-1) selected by the Q-network\n",
        "    outputs: next_state - new state (x, y) location of arm\n",
        "             reward - reward returned by Sawyer environment\n",
        "             done - boolean whether environment is solved'''\n",
        "\n",
        "    # maps actions selected by Q-network to Sawyer arm actions\n",
        "    # array MUST be length NUM_ACT\n",
        "    action_dic = {0:[-1, 0], 1:[1, 0], 2:[0, -1], 3:[0, 1]}\n",
        "    # look up which action in Sawyer arm space corresponds to the selected integer action\n",
        "    action_sawyer = np.array(action_dic[action], dtype=np.float32)\n",
        "    # take the action\n",
        "    ob, reward, done, info = Sawyer_Env.step(action_sawyer)\n",
        "    # if rendering is turned on, render the environment\n",
        "    if render:\n",
        "        Sawyer_Env.render(mode='rgb_array')\n",
        "    # check if we're \"close enough\" to declare done\n",
        "    if reward > done_threshold:\n",
        "        done = True\n",
        "\n",
        "    # pull the observed state off\n",
        "    next_state = ob['observation'][0:2]\n",
        "\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "def solve_environment(model, state, goal_state, total_reward, steps_per_episode, render):\n",
        "    '''attempt to solve the Sawyer Arm environment using the current policy'''\n",
        "    \n",
        "    # list for recording what happened in the episode\n",
        "    episode_experience = []\n",
        "    succeeded = False\n",
        "\n",
        "    for t in range(steps_per_episode):\n",
        "      # attempt to solve the state - number of steps given to solve the\n",
        "      # state is equal to the passed argument steps_per_episode.\n",
        "\n",
        "      # ======================== TODO modify code ========================\n",
        "      continue\n",
        "      # forward pass to find action\n",
        "\n",
        "      # take the action - use helper function to convert discrete actions to\n",
        "      # actions in the Sawyer environment\n",
        "\n",
        "      # add to the episode experience (what happened)\n",
        "\n",
        "      # calculate total reward\n",
        "\n",
        "      # update state\n",
        "\n",
        "      # mark that we've finished the episode and succeeded with training\n",
        "\n",
        "      # ========================      END TODO       ========================\n",
        "\n",
        "\n",
        "    return succeeded, episode_experience, total_reward\n",
        "\n",
        "\n",
        "def solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render):\n",
        "    '''attempt to solve the Sawyer Arm environment using the current policy with no goal condition'''\n",
        "    \n",
        "    # list for recording what happened in the episode\n",
        "    episode_experience = []\n",
        "    succeeded = False\n",
        "\n",
        "    for t in range(steps_per_episode):\n",
        "        inputs = state\n",
        "        inputs = np.expand_dims(inputs, axis=0)\n",
        "        inputs = np.array(inputs, dtype=np.float32) \n",
        "        # forward pass to find action\n",
        "        out = model(inputs)\n",
        "        action = np.argmax(out,axis = 1)[0]\n",
        "        next_state,reward,done, _ = take_action(action, render)\n",
        "        # add to the episode experience (what happened)\n",
        "        episode_experience.append((state,action,reward,next_state,goal_state))\n",
        "        # calculate total reward\n",
        "        total_reward += reward\n",
        "        # update state\n",
        "        state = next_state\n",
        "        # mark that we've finished the episode and succeeded with training\n",
        "        if done:\n",
        "            if succeeded:\n",
        "                continue\n",
        "            else:\n",
        "                succeeded = True\n",
        "    else:\n",
        "         env.stats_recorder.save_complete()\n",
        "         env.stats_recorder.done = True\n",
        "\n",
        "    return succeeded, episode_experience, total_reward\n",
        "\n",
        "def update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER):\n",
        "    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n",
        "    buffer. When HER is used, num_relabeled additional relabeled data points are also added\n",
        "    to the replay buffer\n",
        "\n",
        "    inputs:    epsidode_experience - list of transitions from the last episode\n",
        "    modifies:  replay_buffer\n",
        "    outputs:   None'''\n",
        "\n",
        "    for t in range(steps_per_episode) :\n",
        "        # copy actual experience from episode_experience to replay_buffer\n",
        "\n",
        "        # ======================== TODO modify code ========================\n",
        "        s,a,r,s_,g = episode_experience[t]\n",
        "        # state\n",
        "        inputs = s\n",
        "        # next state\n",
        "        inputs_ = s_\n",
        "        # add to the replay buffer\n",
        "        replay_buffer.add(inputs,a,r,inputs_)\n",
        "\n",
        "\n",
        "        # when HER is used, each call to update_replay_buffer should add num_relabeled\n",
        "        # relabeled points to the replay buffer per step\n",
        "        if HER == 'None':\n",
        "            # HER not being used, so do nothing\n",
        "            pass\n",
        "\n",
        "        elif HER == 'final':\n",
        "            # final - relabel based on final state in episode\n",
        "            pass\n",
        "\n",
        "        elif HER == 'future':\n",
        "            # future - relabel based on future state. At each timestep t, relabel the\n",
        "            # goal with a randomly select timestep between t and the end of the\n",
        "            # episode\n",
        "            pass\n",
        "\n",
        "        elif HER == 'random':\n",
        "            # random - relabel based on a random state in the episode\n",
        "            pass\n",
        "\n",
        "\n",
        "        # ========================      END TODO       ========================\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid value for Her flag - HER not used\")\n",
        "    return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX0tbP_w9ViT"
      },
      "source": [
        "# ************   Main Training Loop    ************ #\n",
        "\n",
        "def run_sawyer(num_epochs, buffer_size = 1e6, batch_size = 128, \n",
        "               num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40,\n",
        "               steps_per_episode=50, render=False, HER = \"None\"):\n",
        "    '''Main loop for running in the Sawyer environment. The DQN is\n",
        "    trained over num_epochs. In each epoch, the agent runs in the environment\n",
        "    num_episodes number of times. The Q-target and Q-policy networks are\n",
        "    updated at the end of each epoch. Within one episode, Q-policy attempts\n",
        "    to solve the environment and is limited to the same number as steps as the\n",
        "    size of the environment\n",
        "\n",
        "    inputs: HER - string specifying whether to use HER'''\n",
        "    # create Sawyer arm environment and replay buffer\n",
        "    replay_buffer = Buffer(buffer_size,batch_size)\n",
        "\n",
        "    # set up Q-policy (model) and Q-target (target_model)\n",
        "    model = Model(NUM_ACT)\n",
        "    target_model = Model(NUM_ACT)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
        "\n",
        "    # ======================== TODO modify code ========================\n",
        "    # modify to be goal conditioned\n",
        "    reset_state = Sawyer_Env.reset()  \n",
        "    state = reset_state['observation'][:2]          # look up the state\n",
        "    \n",
        "    inputs = np.expand_dims(state, axis=0)  \n",
        "    model(inputs)\n",
        "    target_model(inputs)\n",
        "\n",
        "    # start by making Q-target and Q-policy the same\n",
        "    updateTarget(model, target_model, tau=0.0)\n",
        "\n",
        "    # ========================      END TODO       ========================\n",
        "\n",
        "    total_loss = []                  # training loss for each epoch\n",
        "    success_rate = []                # success rate for each epoch\n",
        "    \n",
        "    for i in range(num_epochs):\n",
        "        # Run for a fixed number of epochs\n",
        "\n",
        "        total_reward = 0.0           # total reward for the epoch\n",
        "        successes = []               # record success rate for each episode of the epoch\n",
        "        losses = []                  # loss at the end of each epoch\n",
        "\n",
        "        for k in range(num_episodes):\n",
        "            reset_state = Sawyer_Env.reset()                # reset the environment\n",
        "            state = reset_state['observation'][:2]          # look up the state\n",
        "            goal_state = reset_state['desired_goal'][:2]    # look up the goal\n",
        "\n",
        "            # attempt to solve the environment\n",
        "            # ======================== TODO modify code ========================\n",
        "            # modify to be goal conditioned\n",
        "            succeeded, episode_experience, total_reward = solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render)\n",
        "            # ========================      END TODO       ========================\n",
        "\n",
        "            successes.append(succeeded)                     # track whether we succeeded in environment \n",
        "            update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n",
        "            env.close() \n",
        "            glfw.terminate()\n",
        "        for k in range(opt_steps):\n",
        "            # optimize the Q-policy network\n",
        "\n",
        "            # sample from the replay buffer\n",
        "            state,action,reward,next_state = replay_buffer.sample()\n",
        "            state = np.array(state, dtype=np.float32) \n",
        "            next_state = np.array(next_state, dtype=np.float32) \n",
        "            # forward pass through target network   \n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "              target_net_Q = target_model(next_state)\n",
        "              # calculate target reward\n",
        "              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n",
        "              # calculate loss\n",
        "              model_predict = model(state)\n",
        "              model_action_taken = np.reshape(action,[-1])\n",
        "              action_one_hot = tf.one_hot(model_action_taken, NUM_ACT)\n",
        "              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n",
        "              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n",
        "              losses.append(loss)\n",
        "            \n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n",
        "        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n",
        "    return success_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMNnVQsNAgni"
      },
      "source": [
        "success_rate = run_sawyer(num_epochs=150, HER = \"None\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbhq3UiESmon"
      },
      "source": [
        "# If you chose to render:\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UKXENISPtf5"
      },
      "source": [
        "# Plotting Code\n",
        "\n",
        "We've provided some sample plotting code for you. Feel free to customize it per the assignment specifications. The code will not be graded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOzYeO5EAo6c"
      },
      "source": [
        "pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrP-V9ZJHW-3"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from plotly import graph_objs as go\n",
        "\n",
        "# Sample plotting clode (replace successes where necessary)\n",
        "exp_to_accuracies = {\n",
        "    \"bitflip_7_her_none\": success_rate,\n",
        "    \"bitflip_7_her_ final\": success_rate\n",
        "}\n",
        "\n",
        "# Creates the Figure\n",
        "fig = go.Figure()\n",
        "data = []\n",
        "for experiment, accuracies in exp_to_accuracies.items():\n",
        "  steps = range(len(accuracies))\n",
        "  steps = [5 * x for x in steps]\n",
        "  data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n",
        "\n",
        "# Applies a custom layout\n",
        "layout = go.Layout(\n",
        "    title=go.layout.Title(\n",
        "        text='Bitflip with 7 bits',\n",
        "        x=0.5\n",
        "    ),\n",
        "    xaxis=go.layout.XAxis(\n",
        "        title=go.layout.xaxis.Title(\n",
        "            text='Epoch',\n",
        "            font=dict(\n",
        "                family='Courier New, monospace',\n",
        "                size=18,\n",
        "                color='#7f7f7f'\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        "    yaxis=go.layout.YAxis(\n",
        "        title=go.layout.yaxis.Title(\n",
        "            text='Success Rate',\n",
        "            font=dict(\n",
        "                family='Courier New, monospace',\n",
        "                size=18,\n",
        "                color='#7f7f7f'\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "HTML(fig.to_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqMV9wr0Jdn9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}