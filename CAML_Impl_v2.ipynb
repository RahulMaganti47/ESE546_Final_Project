{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAML Impl v2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+3+azo5TXXfe1raWGTdEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheilsarda/ESE546_Final_Project/blob/master/CAML_Impl_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU02eDlC0_lh",
        "outputId": "e16cb1cd-48e1-4da6-8263-94f74d83b5eb"
      },
      "source": [
        "#@title Mount Google Drive\r\n",
        "#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \r\n",
        "#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\r\n",
        "\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "#@title set up mount symlink\r\n",
        "\r\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\r\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\r\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\r\n",
        "  %mkdir $DRIVE_PATH\r\n",
        "\r\n",
        "## the space in `My Drive` causes some issues,\r\n",
        "## make a symlink to avoid this\r\n",
        "SYM_PATH = '/content/cs330_fall2020'\r\n",
        "if not os.path.exists(SYM_PATH):\r\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZwCtnkr1dmt",
        "outputId": "c10b3127-8db5-486c-8b8a-86e663fc22d2"
      },
      "source": [
        "#@title Install Requirements\r\n",
        "#@markdown Requirements for the assignment and display drivers\r\n",
        "\r\n",
        "# Robot sim\r\n",
        "!pip install gym==0.15.4\r\n",
        "!pip install pygame\r\n",
        "\r\n",
        "# Various things for render\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "# Rendering Environment\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install piglet\r\n",
        "!sudo apt-get install -y xvfb ffmpeg\r\n",
        "!pip install imageio\r\n",
        "!pip install PILLOW\r\n",
        "\r\n",
        "# Commented out IPython magic to ensure Python compatibility.\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "#@title Download Mujoco from an online repository\r\n",
        "\r\n",
        "MJC_PATH = '{}/mujoco'.format(SYM_PATH)\r\n",
        "if not os.path.exists(MJC_PATH):\r\n",
        "  %mkdir $MJC_PATH\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\r\n",
        "  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\r\n",
        "  !unzip -q mujoco200_linux.zip\r\n",
        "  %mv mujoco200_linux mujoco200\r\n",
        "  %rm mujoco200_linux.zip\r\n",
        "\r\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\r\n",
        "os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\r\n",
        "\r\n",
        "## installation on colab does not find *.so files\r\n",
        "## in LD_LIBRARY_PATH, copy over manually instead\r\n",
        "!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/\r\n",
        "\r\n",
        "#@title Important system updates for mujoco-py\r\n",
        "!apt update \r\n",
        "!apt install -y --no-install-recommends \\\r\n",
        "        build-essential \\\r\n",
        "        curl \\\r\n",
        "        git \\\r\n",
        "        gnupg2 \\\r\n",
        "        make \\\r\n",
        "        cmake \\\r\n",
        "        ffmpeg \\\r\n",
        "        swig \\\r\n",
        "        libz-dev \\\r\n",
        "        unzip \\\r\n",
        "        zlib1g-dev \\\r\n",
        "        libglfw3 \\\r\n",
        "        libglfw3-dev \\\r\n",
        "        libxrandr2 \\\r\n",
        "        libxinerama-dev \\\r\n",
        "        libxi6 \\\r\n",
        "        libxcursor-dev \\\r\n",
        "        libgl1-mesa-dev \\\r\n",
        "        libgl1-mesa-glx \\\r\n",
        "        libglew-dev \\\r\n",
        "        libosmesa6-dev \\\r\n",
        "        lsb-release \\\r\n",
        "        ack-grep \\\r\n",
        "        patchelf \\\r\n",
        "        wget \\\r\n",
        "        xpra \\\r\n",
        "        xserver-xorg-dev \\\r\n",
        "        xvfb \\\r\n",
        "        python-opengl \\\r\n",
        "        ffmpeg > /dev/null 2>&1\r\n",
        "\r\n",
        "#@title Clone and install mujoco-py\r\n",
        "#@markdown Remember that you need to put the key in the appropriate location as described above\r\n",
        "%cd $MJC_PATH\r\n",
        "if not os.path.exists('mujoco-py'):\r\n",
        "  !git clone https://github.com/openai/mujoco-py.git\r\n",
        "%cd mujoco-py\r\n",
        "%pip install -e .\r\n",
        "\r\n",
        "## cythonize at the first import\r\n",
        "import mujoco_py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 55.2MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp36-none-any.whl size=1648486 sha256=ea3a0682ba95051a42be7da1bd4e967fe7aa096b88632a978cb8b55e5ee3411a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/4c/2ebe8ab1a695a446574bc48d96eb3503649893be8c769e7fafd65fd18833/pygame-2.0.0-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5MB 219kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (764 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (1,252 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 147220 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "/content/gdrive/My Drive/cs330_fall2020/mujoco\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,814 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,372 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [237 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,699 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,243 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,136 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [266 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 11.1 MB in 3s (3,951 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWtQ6eKttUfw"
      },
      "source": [
        "!apt-get -qq -y install libnvtoolsext1 > /dev/null\r\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\r\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\r\n",
        "!pip -q install gym\r\n",
        "!pip -q install pyglet\r\n",
        "!pip -q install pyopengl\r\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINRUIpSpqZV"
      },
      "source": [
        "\"\"\"\r\n",
        "Imports\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "import gym\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import argparse \r\n",
        "import time \r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import deque\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import random\r\n",
        "from gym import wrappers\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1024, 768))\r\n",
        "display.start()\r\n",
        "import os\r\n",
        "# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\r\n",
        "\r\n",
        "import matplotlib.animation\r\n",
        "import numpy as np\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "# Note, you may get a few warnings regarding Tensorflow and xdpyinfo, these are to be expected\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\r\n",
        "from random import shuffle\r\n",
        "import sys\r\n",
        "from copy import deepcopy\r\n",
        "import warnings\r\n",
        "import math, random\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import torch \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.autograd as autograd  \r\n",
        "import torch.nn.functional as F\r\n",
        "import abc\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import distributions as td\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch\r\n",
        "import torch.nn as nn \r\n",
        "import torch.nn.functional as F \r\n",
        "import torch.optim as optim\r\n",
        "from torch.distributions import Categorical\r\n",
        "import gym\r\n",
        "from collections import deque\r\n",
        "import numpy as np \r\n",
        "from torch.distributions import Categorical\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.distributions import Normal \r\n",
        "import os \r\n",
        "import os\r\n",
        "import gym\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40) # error only\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "import glob\r\n",
        "import io\r\n",
        "import base64\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\r\n",
        "    !bash ../xvfb start\r\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9zLBRQ9334n"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "eps = 1e-6\r\n",
        "min_std = -20\r\n",
        "max_std = 2\r\n",
        "\r\n",
        "class Actor(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(Actor, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3mu = nn.Linear(256, action_space)\r\n",
        "        self.fc3std = nn.Linear(256, action_space)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        z = F.relu(self.fc1(x))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        mu = self.fc3mu(z)\r\n",
        "        std = self.fc3std(z)\r\n",
        "        std = torch.clamp(std, min_std, max_std)\r\n",
        "        return mu, std\r\n",
        "\r\n",
        "class QCritic(nn.Module):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space):\r\n",
        "        super(QCritic, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(state_space+action_space, 256)\r\n",
        "        self.fc2 = nn.Linear(256, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 1)\r\n",
        "    \r\n",
        "    \r\n",
        "    def forward(self, state, action):\r\n",
        "        z = torch.cat([state, action], 1)\r\n",
        "        z = F.relu(self.fc1(z))\r\n",
        "        z = F.relu(self.fc2(z))\r\n",
        "        z = self.fc3(z)\r\n",
        "        return z\r\n",
        "\r\n",
        "\r\n",
        "class SAC:\r\n",
        "\r\n",
        "    def __init__(self, state_space, action_space, hp=None, name='SAC'): \r\n",
        "        self.name = name\r\n",
        "        if hp is None:\r\n",
        "            self.tau = 0.005\r\n",
        "            self.lr = 3*(1e-4)\r\n",
        "            self.batch_size = 256\r\n",
        "            self.gamma = 0.99\r\n",
        "            self.max_action = 1\r\n",
        "            self.min_action = 0\r\n",
        "        else:\r\n",
        "            self.tau = hp['tau'] \r\n",
        "            self.lr = hp['lr'] \r\n",
        "            self.batch_size = hp['batch_size']\r\n",
        "            self.gamma = hp['gamma']\r\n",
        "            self.max_action = hp['max_action']\r\n",
        "            self.min_action = hp['min_action']\r\n",
        "        self.actor = Actor(state_space, action_space).to(device)\r\n",
        "        self.qcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.qcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic = QCritic(state_space, action_space).to(device)\r\n",
        "        self.tcritic2 = QCritic(state_space, action_space).to(device)\r\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\r\n",
        "        self.target_ent = -action_space\r\n",
        "        print(\"Target Entropy: {}\".format(self.target_ent))\r\n",
        "        print(\"Verify Device: {}\".format(device))\r\n",
        "        self.act_opt = torch.optim.Adam(params=self.actor.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt = torch.optim.Adam(params=self.qcritic.parameters(), lr=self.lr)\r\n",
        "        self.qc_opt2 = torch.optim.Adam(params=self.qcritic2.parameters(), lr=self.lr)\r\n",
        "        self.alpha_opt = torch.optim.Adam([self.log_alpha], lr=self.lr)\r\n",
        "        self.action_scale = torch.tensor((self.max_action-self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "        self.action_bias = torch.tensor((self.max_action+self.min_action)/2., dtype=torch.float32).to(device)\r\n",
        "\r\n",
        "    def predict(self, x, pred=True, internalCall=False, test=False):\r\n",
        "        if test:\r\n",
        "            self.actor.eval()\r\n",
        "        if pred and not internalCall:\r\n",
        "            x = torch.from_numpy(x).float().to(device)\r\n",
        "        if pred:\r\n",
        "            with torch.no_grad():\r\n",
        "                #self.actor.eval()\r\n",
        "                mu, std = self.actor(x)\r\n",
        "            #print(\"Mu, Log_Std: {} {}\".format(mu, std))\r\n",
        "            #self.get_actor_mean()\r\n",
        "        else:\r\n",
        "            self.actor.train()\r\n",
        "            mu, std = self.actor(x)\r\n",
        "        \r\n",
        "        #Treat initial output std as log_std - prevent <= 0 std\r\n",
        "        std = std.exp()\r\n",
        "        act_dist = Normal(mu, std)\r\n",
        "        \r\n",
        "        u = act_dist.rsample()\r\n",
        "        action = F.tanh(u)*self.action_scale+self.action_bias\r\n",
        "        log_prob = act_dist.log_prob(u)\r\n",
        "        jacobian = torch.log((1-torch.square(action))+eps)\r\n",
        "        #if internalCall:\r\n",
        "        jacobian = jacobian.sum(1, keepdim=True)\r\n",
        "        log_prob -= jacobian\r\n",
        "        \r\n",
        "        if test:\r\n",
        "            action = F.tanh(mu)*self.action_scale+self.action_bias\r\n",
        "            self.actor.train()\r\n",
        "        #  Internalcall used in training, evaluation and data collection has default False\r\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\r\n",
        "        if internalCall:\r\n",
        "            return action, log_prob\r\n",
        "        else:\r\n",
        "            #print(\"Action: {} State: {}\".format(action, x))\r\n",
        "            #self.get_actor_mean()\r\n",
        "            return np.squeeze(action.cpu().numpy()), np.squeeze(log_prob.cpu().numpy())\r\n",
        "\r\n",
        "\r\n",
        "    def _update_target(self):\r\n",
        "        for q1, q1t in zip(self.qcritic.parameters(), self.tcritic.parameters()):\r\n",
        "            q1t.data *= (1-self.tau)\r\n",
        "            q1t.data += (self.tau)*q1.data\r\n",
        "        for q2, q2t in zip(self.qcritic2.parameters(), self.tcritic2.parameters()):\r\n",
        "            q2t.data *= (1-self.tau)\r\n",
        "            q2t.data += self.tau*q2.data\r\n",
        "\r\n",
        "\r\n",
        "    def train_step(self, replay_buffer, batch_size):\r\n",
        "        state_set, action_set, reward_set, nstate_set, logprob_set, done_set = replay_buffer.sample(self.batch_size)\r\n",
        "        state_set = torch.from_numpy(state_set).float().to(device)\r\n",
        "        action_set = torch.from_numpy(action_set).float().to(device)\r\n",
        "        reward_set = torch.from_numpy(reward_set).float().to(device)\r\n",
        "        nstate_set = torch.from_numpy(nstate_set).float().to(device)\r\n",
        "        done_set = torch.from_numpy(done_set).float().to(device)\r\n",
        "        logprob_set = torch.from_numpy(logprob_set).float().to(device)\r\n",
        "       \r\n",
        "        alpha = self.log_alpha.exp()\r\n",
        "        act, log_prob = self.predict(nstate_set, internalCall=True)\r\n",
        "        qOut = torch.min(self.tcritic(nstate_set, act).detach(), self.tcritic2(nstate_set, act).detach())\r\n",
        "        qOut -= alpha.detach()*(log_prob)\r\n",
        "        reward_set = torch.unsqueeze(reward_set, 1)\r\n",
        "        done_set = torch.unsqueeze(done_set, 1)\r\n",
        "        target = reward_set+(1-done_set)*self.gamma*qOut\r\n",
        "        q1loss = F.mse_loss(target, self.qcritic(state_set, action_set))\r\n",
        "        q2loss = F.mse_loss(target, self.qcritic2(state_set, action_set))\r\n",
        "        \r\n",
        "        self.qc_opt.zero_grad()\r\n",
        "        q1loss.backward()\r\n",
        "        self.qc_opt.step()\r\n",
        "        self.qc_opt2.zero_grad()\r\n",
        "        q2loss.backward()\r\n",
        "        self.qc_opt2.step()\r\n",
        "\r\n",
        "        self.act_opt.zero_grad()\r\n",
        "        act, log_prob = self.predict(state_set, False, True)\r\n",
        "        min_q = torch.min(self.qcritic(state_set, act), self.qcritic2(state_set, act))\r\n",
        "        actor_loss = (alpha.detach()*log_prob-min_q).mean() \r\n",
        "        actor_loss.backward()\r\n",
        "        self.act_opt.step()\r\n",
        "        \r\n",
        "        self.alpha_opt.zero_grad()\r\n",
        "        alphaLoss = (-self.log_alpha*(log_prob+self.target_ent).detach()).mean()\r\n",
        "        alphaLoss.backward()\r\n",
        "        self.alpha_opt.step()\r\n",
        "        self._update_target()\r\n",
        "        #print(\"Loss List: q1 {}, q2 {}, act {}, alpha {}\".format(q1loss, q2loss, actor_loss, alphaLoss)) \r\n",
        "        #self.get_actor_mean()\r\n",
        "\r\n",
        "    def save(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "            if os.path.isdir('models') is False:\r\n",
        "                os.mkdir('models')\r\n",
        "        torch.save({\r\n",
        "            'actor': self.actor.state_dict(),\r\n",
        "            'qcritic': self.qcritic.state_dict(),\r\n",
        "            'qcritic2': self.qcritic2.state_dict(),\r\n",
        "            'tcritic': self.tcritic.state_dict(),\r\n",
        "            'tcritic2': self.tcritic.state_dict(),\r\n",
        "            'log_alpha': self.log_alpha,\r\n",
        "            'qopt': self.qc_opt.state_dict(),\r\n",
        "            'qopt2': self.qc_opt2.state_dict(),\r\n",
        "            'actOpt': self.act_opt.state_dict(),\r\n",
        "            'alphaOpt': self.alpha_opt.state_dict()\r\n",
        "            }, path) \r\n",
        "    \r\n",
        "\r\n",
        "    def load(self, path=None):\r\n",
        "        if path is None:\r\n",
        "            path = 'models/{}'.format(self.name)\r\n",
        "        load_dict = torch.load(path)\r\n",
        "        self.actor.load_state_dict(load_dict['actor'])\r\n",
        "        self.qcritic.load_state_dict(load_dict['qcritic'])\r\n",
        "        self.qcritic2.load_state_dict(load_dict['qcritic2'])\r\n",
        "        self.tcritic.load_state_dict(load_dict['tcritic'])\r\n",
        "        self.tcritic2.load_state_dict(load_dict['tcritic2'])\r\n",
        "        self.log_alpha = load_dict['log_alpha']\r\n",
        "        self.qc_opt.load_state_dict(load_dict['qopt'])\r\n",
        "        self.qc_opt2.load_state_dict(load_dict['qopt2'])\r\n",
        "        self.act_opt.load_state_dict(load['actOpt'])\r\n",
        "        self.alpha_opt.load_state_dict(load_dict['alphaOpt']) \r\n",
        "    \r\n",
        "    def get_actor_mean(self):\r\n",
        "        print(\"Start\")\r\n",
        "        for name, p in self.actor.named_parameters():\r\n",
        "            print(name)\r\n",
        "            print(p.data.mean())\r\n",
        "            print(\"Gradient Data\")\r\n",
        "            print(p.grad.data.mean())\r\n",
        "        print(\"End\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs_q1dW80yv7"
      },
      "source": [
        "class ReplayMemory(object):\r\n",
        "\r\n",
        "    def __init__(self, length = 10000):\r\n",
        "    \r\n",
        "        # Buffer Collection: (S, A, R, S', log_prob (if available),  D)\r\n",
        "        # Done represents a mask of either 0 and 1\r\n",
        "        self.length = length\r\n",
        "        self.buffer = []\r\n",
        "\r\n",
        "    def add(self, sample):\r\n",
        "\r\n",
        "        if (len(self.buffer) > self.length):\r\n",
        "            self.buffer.pop(0)\r\n",
        "        self.buffer.append(sample)\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        \r\n",
        "        idx = np.random.permutation(len(self.buffer))[:batch_size]\r\n",
        "        state_b = []\r\n",
        "        action_b = []\r\n",
        "        reward_b = []\r\n",
        "        nextstate_b = []\r\n",
        "        done_b = [] \r\n",
        "        log_prob = []\r\n",
        "        for i in idx:\r\n",
        "            if (len(self.buffer[0])==5):\r\n",
        "                s, a, r, sp, d = self.buffer[i]\r\n",
        "            else:\r\n",
        "                s, a, r, sp, d, lp = self.buffer[i]\r\n",
        "                log_prob.append(lp)\r\n",
        "            state_b.append(s)\r\n",
        "            action_b.append(a)\r\n",
        "            reward_b.append(r)\r\n",
        "            nextstate_b.append(sp)\r\n",
        "            done_b.append(d)\r\n",
        "        state_b = np.array(state_b)\r\n",
        "        action_b = np.array(action_b)\r\n",
        "        reward_b = np.array(reward_b)\r\n",
        "        nextstate_b = np.array(nextstate_b)\r\n",
        "        done_b = np.array(done_b)\r\n",
        "        if len(self.buffer[0]) == 5:\r\n",
        "            print(\"Red Flag\")\r\n",
        "            return (state_b, action_b, reward_b, nextstate_b, done_b)\r\n",
        "        else:\r\n",
        "            return (state_b, action_b, reward_b, nextstate_b, done_b, np.array(log_prob))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Hml1od5taD"
      },
      "source": [
        "def clog_prob(val, mu=0, std=1):\r\n",
        "    return np.sum(-np.log(np.sqrt(2*np.pi))-np.log(std**2)-(1/2*std)*np.square(val-mu))\r\n",
        "\r\n",
        "def process_state(state):\r\n",
        "    \r\n",
        "    #state /= np.sqrt(np.sum(np.square(state)))\r\n",
        "    return state\r\n",
        "\r\n",
        "def fillBuffer(env, memory, action_space, length=1000, returnlp=True):\r\n",
        "    \r\n",
        "    state = process_state(env.reset())\r\n",
        "    for _ in range(length):\r\n",
        "        action = np.random.normal(size=action_space)\r\n",
        "        if returnlp:\r\n",
        "            log_prob = clog_prob(action)\r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "        new_state = process_state(new_state)\r\n",
        "        if returnlp:\r\n",
        "            memory.add((state, action, reward, new_state, log_prob, done))\r\n",
        "        else:\r\n",
        "            memory.add((state, action, reward, new_state, done))\r\n",
        "        state = new_state\r\n",
        "        if done:\r\n",
        "            state = process_state(env.reset())\r\n",
        "\r\n",
        "def recordEps(env, model, recordEps=True, save_path=None):\r\n",
        "\r\n",
        "    #  Records episode run by default, otherwise acts as a test run for the model\r\n",
        "    if recordEps:\r\n",
        "        rec = gym.wrappers.monitoring.video_recorder.VideoRecorder(env, save_path)\r\n",
        "    state = env.reset()\r\n",
        "    r = 0\r\n",
        "    step = 0\r\n",
        "    while(True and step <= 5000):\r\n",
        "        tup = model.predict(np.expand_dims(state, axis=0), test=True)\r\n",
        "        if (type(tup) is tuple):\r\n",
        "            action, _ = tup\r\n",
        "        else:\r\n",
        "            action = tup\r\n",
        "        if recordEps:\r\n",
        "            rec.capture_frame()\r\n",
        "        state, reward, done, _ = env.step(action)\r\n",
        "        r += reward\r\n",
        "        step += 1\r\n",
        "        if done:\r\n",
        "            if recordEps:\r\n",
        "                rec.close()\r\n",
        "            print(\"Reward at termination: {}\".format(r))\r\n",
        "            print(\"Avg Reward: {}\".format(r/step))\r\n",
        "            return\r\n",
        "\r\n",
        "def baseEp(env):\r\n",
        "\r\n",
        "    state = env.reset()\r\n",
        "    r = 0\r\n",
        "    while(True):\r\n",
        "        action = np.random.normal(size = env.action_space.shape[0])\r\n",
        "        # print(\"Generated random action\")\r\n",
        "        state, reward, done, _ = env.step(action)\r\n",
        "        r += reward\r\n",
        "        # print(\"Reward: \" + str(reward))\r\n",
        "        if done:\r\n",
        "            print(\"Random: Reward at Termination: {}\".format(r))\r\n",
        "            return\r\n",
        "\r\n",
        "\r\n",
        "def runEp(env, memory, model, returnlp=False, returnReward=False):\r\n",
        "     \r\n",
        "    step = 0\r\n",
        "    epochReward = []\r\n",
        "    stepList = []\r\n",
        "    epReward = 0\r\n",
        "    state = process_state(env.reset())\r\n",
        "    while not done: # Note that 5000 was an arbitrary choice\r\n",
        "        if not returnlp:\r\n",
        "            action = model.predict(np.expand_dims(state, axis=0))\r\n",
        "        else:\r\n",
        "            action, log_prob = model.predict(np.expand_dims(state, axis=0))\r\n",
        "        \r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "        new_state = process_state(new_state)\r\n",
        "        \r\n",
        "        #  Add transition tuple to replay buffer\r\n",
        "        if not returnlp:\r\n",
        "            memory.add((state, action, reward, new_state, done))\r\n",
        "        else:\r\n",
        "            memory.add((state, action, reward, new_state, log_prob, done))\r\n",
        "        state = new_state\r\n",
        "        step += 1\r\n",
        "        \r\n",
        "        #  Train every 50 steps\r\n",
        "        if (step%50) == 0:\r\n",
        "            for _ in range(50):\r\n",
        "                model.train_step(memory, batch_size) \r\n",
        "        if returnReward:\r\n",
        "            epReward += reward\r\n",
        "        if done:\r\n",
        "            state = process_state(env.reset())\r\n",
        "            done = False\r\n",
        "            if returnReward:\r\n",
        "                stepList.append(step)\r\n",
        "                epochReward.append(epReward)\r\n",
        "                epReward = 0\r\n",
        "        \r\n",
        "        if (step > 4000): \r\n",
        "            break \r\n",
        "    \r\n",
        "    if returnReward:\r\n",
        "        return stepList, epochReward, action\r\n",
        "    else: \r\n",
        "        return step\r\n",
        "\r\n",
        "def train_model(env, memory, model, epIter):\r\n",
        "    reward_plot = [] # Format of (time step, ep_reward) pairs\r\n",
        "    tStep = 0\r\n",
        "    for i in range(epIter):\r\n",
        "        if ((i+1)%10 == 0): \r\n",
        "            stepList, epochReward, act = runEp(env, memory, model, True, returnReward=True) \r\n",
        "            step = stepList[-1]\r\n",
        "            print(\"Step \" + str(step))\r\n",
        "            for j in range(len(stepList)):\r\n",
        "                stepList[j] += tStep\r\n",
        "            reward_plot.extend([[a,b] for a,b in zip(stepList, epochReward)])\r\n",
        "        else:\r\n",
        "            step = runEp(env, memory, model, returnlp=True, returnReward=False)\r\n",
        "            print(\"here\" + str(step))\r\n",
        "        if ((i+1)%20 == 0):\r\n",
        "            print(\"Last action on last episode: {}\".format(act))\r\n",
        "            print(\"Runtime (hours) at checkpoint: {}\".format((time.time()-start_time)/3600))\r\n",
        "        \r\n",
        "            model.save()\r\n",
        "        tStep += step\r\n",
        "    return reward_plot        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJmLbDWozVT_"
      },
      "source": [
        "class Policy(nn.Module): \r\n",
        "    def __init__(self, s_size=6, h_size=50, a_size=3):\r\n",
        "        super(Policy, self).__init__()\r\n",
        "        self.l1 = nn.Linear(s_size, h_size)\r\n",
        "        self.l2 = nn.Linear(h_size, a_size)\r\n",
        "\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            self.l1, \r\n",
        "            nn.ReLU(), \r\n",
        "            self.l2, \r\n",
        "            nn.Softmax(dim=1)\r\n",
        "        )\r\n",
        "         \r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x)\r\n",
        "    \r\n",
        "    def act(self, state):\r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\r\n",
        "        probs = self.forward(state)\r\n",
        "        m = Categorical(probs)\r\n",
        "        action = m.sample()\r\n",
        "        return action.item() - 1, m.log_prob(action)\r\n",
        "\r\n",
        "def compute_rewards(rewards, gamma):\r\n",
        "    discounted_rewards = np.zeros(len(rewards))\r\n",
        "    moving_add = 0\r\n",
        "    for i in reversed(range(0, len(rewards))):\r\n",
        "        moving_add = moving_add*gamma + rewards[i]\r\n",
        "        discounted_rewards[i] = moving_add\r\n",
        "\r\n",
        "    return discounted_rewards\r\n",
        "\r\n",
        "class Critic(nn.Module): \r\n",
        "    def __init__(self, state_dim=6, hidden_dim=20, output_dim=1, lambd=.9):\r\n",
        "        super(Critic, self).__init__()\r\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim, bias=False)\r\n",
        "        self.l2 = nn.Linear(hidden_dim, output_dim, bias=False)\r\n",
        "        self.lambd = lambd\r\n",
        "\r\n",
        "    def forward(self, state): \r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0) \r\n",
        "        x = F.relu(self.l1(state)) \r\n",
        "        x = self.l2(x) \r\n",
        "        return F.softmax(x, dim=1)\r\n",
        "\r\n",
        "    def td_error(self, reward, value_next, value_now, gamma, done, I): \r\n",
        "        if done: I = I * gamma \r\n",
        "        td_error = reward + gamma*(1-done)*value_next - value_now \r\n",
        "        return td_error \r\n",
        "        \r\n",
        "def train(n_episodes, policy, critic, gamma, print_every=4):\r\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\r\n",
        "    optimizer_v = optim.Adam(critic.parameters(), lr=.001)\r\n",
        "    scores_deque = deque(maxlen=100)\r\n",
        "    \r\n",
        "    total_rewards = []\r\n",
        "    for ep in range(n_episodes): \r\n",
        "        traj_log_probs = []\r\n",
        "        rewards = []\r\n",
        "        state = env.reset()\r\n",
        "        score = 0 \r\n",
        "        I = 1.0 \r\n",
        "        done = False\r\n",
        " \r\n",
        "        while not done:\r\n",
        "            action, log_prob = policy.act(state)\r\n",
        "            #value_func = critic.forward(state)\r\n",
        "            traj_log_probs.append(log_prob)\r\n",
        "            next_state, reward, done, _ = env.step(action)\r\n",
        "            #value_func_next = critic(next_state)\r\n",
        "            #td_error = critic.td_error(reward, value_func_next, value_func, gamma, done, I) \r\n",
        "            \r\n",
        "            score += reward\r\n",
        "            rewards.append(reward)\r\n",
        "           \r\n",
        "        scores_deque.append(score) \r\n",
        "        total_rewards.append(score)\r\n",
        "                \r\n",
        "        disc_rewards = compute_rewards(rewards, gamma)\r\n",
        "        disc_rewards = torch.tensor(disc_rewards)\r\n",
        "        \r\n",
        "        policy_loss = [] \r\n",
        "        for t, log_prob in enumerate(traj_log_probs):\r\n",
        "            policy_loss.append(-log_prob * disc_rewards[t])  \r\n",
        "        policy_loss = torch.cat(policy_loss).sum() \r\n",
        "        \r\n",
        "        #value_loss = F.l1_loss(value, torch.tensor([disc_rewards]))\r\n",
        "        #add gradient trace \r\n",
        "        #for p in critic.parameters(): \r\n",
        "        #    p.grad = p.grad * critic.lambd\r\n",
        "        \r\n",
        "        #loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\r\n",
        "\r\n",
        "        # backprop\r\n",
        "        optimizer.zero_grad()\r\n",
        "        policy_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        if ep % print_every == 0:\r\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_deque)))        \r\n",
        "    \r\n",
        "    return total_rewards\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEK2oAl9-kpQ"
      },
      "source": [
        "from gym.wrappers import Monitor\r\n",
        "def wrap_env(env):\r\n",
        "    env = Monitor(env, './video', force=True)\r\n",
        "    return env\r\n",
        "\r\n",
        "def show_video():\r\n",
        "    mp4list = glob.glob('video/*.mp4')\r\n",
        "    if len(mp4list) > 0:\r\n",
        "        mp4 = mp4list[0]\r\n",
        "        video = io.open(mp4, 'r+b').read()\r\n",
        "        encoded = base64.b64encode(video)\r\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n",
        "                                                loop controls style=\"height: 400px;\">\r\n",
        "                                                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n",
        "                                            </video>'''.format(encoded.decode('ascii'))))\r\n",
        "    else: \r\n",
        "        print(\"Could not find video\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1GeLoo7uMz",
        "outputId": "c156d92a-cc94-4906-f7fa-1be714f33294"
      },
      "source": [
        "start_time = time.time()\r\n",
        "envName = 'HalfCheetah-v2'\r\n",
        "batch_size = 256\r\n",
        "\r\n",
        "print(\"Cuda available: {}\".format(torch.cuda.is_available()))\r\n",
        "if torch.cuda.is_available():\r\n",
        "    print(\"Device Count: {}\".format(torch.cuda.device(0)))\r\n",
        "    print(\"Device Name (first): {}\".format(torch.cuda.get_device_name(0)))\r\n",
        "print(\"Environment Used: {}\".format(envName))\r\n",
        "env = gym.make(envName) \r\n",
        "action_space = env.action_space.shape[0]\r\n",
        "state_space = env.observation_space.shape[0] \r\n",
        "hp = {'tau': 0.005,\\\r\n",
        "        'lr': 3*(1e-4),\\\r\n",
        "        'batch_size': 256,\\\r\n",
        "        'gamma': 0.99,\\\r\n",
        "        'max_action': env.action_space.high,\\\r\n",
        "        'min_action': env.action_space.low\\\r\n",
        "        }\r\n",
        "model = SAC(state_space, action_space, hp=hp, name='DDPG_halfCheetahv2')\r\n",
        "\r\n",
        "rmemory = ReplayMemory(int(1e6))\r\n",
        "fillBuffer(env, rmemory, action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available: True\n",
            "Device Count: <torch.cuda.device object at 0x7f7598773ba8>\n",
            "Device Name (first): Tesla P100-PCIE-16GB\n",
            "Environment Used: HalfCheetah-v2\n",
            "Target Entropy: -6\n",
            "Verify Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQT_BDYjATdr",
        "outputId": "4b5b9d9a-9a48-4c3c-8d87-33baf7a6fcd2"
      },
      "source": [
        "reward_plot = train_model(env, rmemory, model, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "here5000\n",
            "here5000\n",
            "here5000\n",
            "here5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wywYhxteIY0"
      },
      "source": [
        "plt.plot([v[0] for v in reward_plot], [v[1] for v in reward_plot])\r\n",
        "plt.title(\"Episode Reward vs TimeStep\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yTFkUHx8QIt",
        "outputId": "b3d9a9e5-b166-4c86-bab5-7855ba146873"
      },
      "source": [
        "matplotlib.use('Agg')\r\n",
        "# env = wrap_env(env)\r\n",
        "\r\n",
        "baseEp(env)\r\n",
        "baseEp(env)\r\n",
        "baseEp(env)\r\n",
        "\r\n",
        "env.close()\r\n",
        "# show_video()\r\n",
        "\r\n",
        "# print(\"End runtime: {} seconds\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random: Reward at Termination: -652.8554101276853\n",
            "Random: Reward at Termination: -712.6995353010171\n",
            "Random: Reward at Termination: -584.7761032867407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLoduY0qLsQ"
      },
      "source": [
        "class DQN_Network(nn.Module):\r\n",
        "    def __init__(self): \r\n",
        "        super(DQN_Network, self).__init__() \r\n",
        "        \r\n",
        "        # __GRU__ \r\n",
        "        \"\"\"\r\n",
        "        self.input_layer = nn.Linear(8, 128)\r\n",
        "        self.hidden_1 = nn.Linear(128, 128)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.hidden_state = torch.tensor(torch.zeros(2,1,32))\r\n",
        "        self.rnn = nn.GRU(128, 32, 2)\r\n",
        "        self.action_head = nn.Linear(31, 5)\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        self.input_layer = nn.Linear(8, 32)\r\n",
        "        self.hidden_1 = nn.Linear(32, 32)\r\n",
        "        self.hidden_2 = nn.Linear(32,31)\r\n",
        "        self.output_layer = nn.Linear(31, 5)\r\n",
        "    \r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        state = state.squeeze()\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = torch.sigmoid(self.input_layer(state))\r\n",
        "        out = torch.tanh(self.hidden_1(out))\r\n",
        "        out, self.hidden_state = self.rnn(out.view(1,-1,128), self.hidden_state.data)\r\n",
        "        out = F.relu(self.hidden_2(out.squeeze()))\r\n",
        "        out = self.action_head(out)\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.sigmoid(self.output_layer(out))\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        out = F.relu(self.input_layer(state))\r\n",
        "        out = F.relu(self.hidden_1(out))\r\n",
        "        out = F.relu(self.hidden_2(out))\r\n",
        "        out = F.relu(self.output_layer(out))\r\n",
        "        \"\"\"\r\n",
        "        return out \r\n",
        "\r\n",
        "class DQN_Agent(): \r\n",
        "    def __init__(self): \r\n",
        "        #initialize target and policy networks \r\n",
        "        self.target_net = DQN_Network()\r\n",
        "        self.policy_net = DQN_Network()\r\n",
        "\r\n",
        "        self.eps_start = .9  \r\n",
        "        self.eps_end = .05\r\n",
        "        self.eps_decay = 200  \r\n",
        "        self.steps_done = 0 \r\n",
        "\r\n",
        "    def select_action(self, state): \r\n",
        "        random_n = random.random() #generate random number\r\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\r\n",
        "            math.exp(-1. * self.steps_done / self.eps_decay)\r\n",
        "        self.steps_done += 1  \r\n",
        "\r\n",
        "        if (random_n < eps_threshold): \r\n",
        "        #take random action (random # betwee 0 and 4)\r\n",
        "            action = torch.tensor([random.randrange(4)]) \r\n",
        "        else: \r\n",
        "        #take the best action  \r\n",
        "            with torch.no_grad(): \r\n",
        "                actions = self.policy_net(state)  \r\n",
        "                action = torch.argmax(actions).view(1, 1) \r\n",
        "                \r\n",
        "        return action.item() \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIX7ac77ujgc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}