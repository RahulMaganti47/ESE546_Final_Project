# ESE546_Final_Project

Few-Shot Meta Q-Learning with Reptile

Deep neural networks work well only when there is a lot of training data present. Moreover, training these networks is often slow. Human intelligence, on the other hand, learns quickly and extrapolates well only from only a small number of data points. 
Learning in a low-data regime remains a key challenge for Deep Learning. In this paper, we wish to quantitatively validate the effectiveness of few-shot learning in the context of Reinforcement Learning. 
By testing the Reptile algorithm in OpenAI's CartPole simulation environment using a Deep Q-Network model, we believe that the results obtained from this context will enhance the studies shown on few-shot learning approaches.
